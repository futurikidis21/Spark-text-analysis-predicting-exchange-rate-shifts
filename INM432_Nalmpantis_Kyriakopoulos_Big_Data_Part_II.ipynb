{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM432: Big Data - Coursework (Part II)\n",
    "\n",
    "## Predicting shifts in GBP-EUR exchange rates based on the content of UK parliamentary debates: A pySpark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexandros Dimitrios Nalmpantis; Georgios Kyriakopoulos (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study objective and approach \n",
    "\n",
    "The pySpark code presented in this notebook aims to construct a modelling process, which tests if shifts in GBP-EUR exchange rates can be predicted on the basis of political speech. More specifically, the code implements a process that involves the following steps:\n",
    "\n",
    "(a) It scrapes the (almost daily) reports that record debates at the House of Commons and the House of Lords, as published on the UK Government website. The reports (available in PDF format) are converted into txt files and common (usually uninformative) words, numeric figures, and punctuation symbols are removed. File names and the remainder text content are converted into a data-frame and the content is tokenised and hashed, before computing term frequencies (TFs) and inverse document frequencies (IDFs) for the hashes.\n",
    "\n",
    "(b) It scrapes the (almost daily) EUR-GBP exchange rates, published on the Bank of England website. The content is written in a text file, which is then processed to remove blank space and to calculate the difference between each GBP-EUR exchange-rate value and its immediately previous exchange-rate value (i.e. *the exchange-rate shift*). Dates and exchange-rate shifts are then converted into a second data-frame.\n",
    "\n",
    "(c) It links the data frames constructed at (a) and (b) together, so that the content of the debate reports at a certain date is appended to the shift in GBP-EUR exchange rates recorded the day after. This new data-frame serves as the analytical input on the basis of which the main analysis is conducted.\n",
    "\n",
    "(d) Finally, it constructs a machine learning pipeline, whereby the linear regression models are trained, validated, and tested to predict exchange-rate shifts based on either the TF or the IDF of the (tokenised and hashed) content of the debate reports. Alternative parameterisation options are explored using a grid.\n",
    "\n",
    "The processes of data collection embedded in (a) and (b) depend on the format at which relevant data are published on the UK Government and the Bank of England websites. For this study, we collect data from 01/06/2016 onwards, and the functionality of the code that collects the data was last confirmed on 23/04/2017. However, future functionality  can be affected by future changes in the way that information is published. ***Along with this notebook, we provide the datafiles scraped on 23/04/2017, so that the analysis can be repeated when this code is used / assessed by others***.  \n",
    "\n",
    "The findings of the analysis are highlighted throughout the annotation in this notebook. Overall (and perhaps unsurprisingly), the findings suggest that political speech at the House of Lords and the House of commons is unsuccessful in predicting complex and dynamic macroeconomic financial-market parameters, such as the GBP-EUR exchange-rate shifts. \n",
    "\n",
    "Given the time-series nature of the data that this analysis considers, we have (partially) addressed the issue of autocorrelations when using linear regression analysis by focusing the analysis on the prediction of exchange-rate shifts (rather than the prediction of exchange rate values). We suggest that the analysis is revisited in the future using perhaps more appropriate analytical techniques, such as Autoregressive Integrated Moving Average time-series models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 || Analysis modules\n",
    "\n",
    "The modules needed for the analysis are imported below. Some modules may need to be installed to the terminal with the following commands: **pip install <\"name of module\">** eg: pip install tqdm  or **with conda install <\"name of module\">** eg: conda install tqdm.\n",
    "\n",
    "The modules that will be required are **BeautifulSoup, urllib, re, wget, tika, tqdm, stop_words**.\n",
    "\n",
    "The folder with the data and the notebook can be found in .zip format on:\n",
    "https://cityuni-my.sharepoint.com/personal/alexandros_nalmpantis_city_ac_uk/_layouts/15/guestaccess.aspx?folderid=1f6589982e68d49499959d64f6a565018&authkey=AQW7rgXzZBcwHZW67Im0-P4\n",
    "\n",
    "Alternatively the data can be scraped by selecting no to the following user inputs regarding the data. In this case the pachages listed above will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8759199dd958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Modules used for for parsing PDFs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtika\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bs4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "# Modules used for scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date,timedelta\n",
    "import os\n",
    "\n",
    "# Modules used for downloads\n",
    "#import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modules used for for parsing PDFs\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Spark ML and SQL modules\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Other modules\n",
    "from math import log\n",
    "import time\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from  stop_words import get_stop_words\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set sparkcontext as sc\n",
    "#sc=SparkContext() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 || Data collection and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     2.1 Procedures for scraping, downloading, and converting parliamentary-debate reports into text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define if readily available debate-report data will be used (trg= 'yes') or if they will be scraped (trg ='no')\n",
    "trg='yes' #change this value to 'yes' or 'no'\n",
    "\n",
    "# Set date from which onwards debate-reports with be downloaded in YYYY-M-D format\n",
    "start_date = date(2016, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data_control function that controls whether the readily available debate-report data will be used or if they will be scraped\n",
    "def data_control(page,start_date,trg):\n",
    "    if trg=='yes':\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        html_page = urllib.request.urlopen(page) #request page with urllib packages\n",
    "        soup = BeautifulSoup(html_page) #pass the page to beautiful soup in order to extract the links contained in webpage \n",
    "        #print (soup) #visually inspect the html structure\n",
    "        hl = [] #set hyperlink array to store the extracted links\n",
    "        ##search html for hyperlinks starting with qna\n",
    "        for hyperlink in soup.findAll('a', attrs={'href': re.compile(\"^http://qna\")}): \n",
    "            hl.append(hyperlink.get('href')) #store the hyperlinks found on an array\n",
    "        #    print (link.get('href'))\n",
    "        \n",
    "        url=[hl[1][:-22]+'Lords-',hl[1][:-22]+'Commons-'] #take first result and cut the dates and category of either lords or commons\n",
    "    \n",
    "        ##create interval search date\n",
    "        today=datetime.datetime.today() #today's date set\n",
    "        cur_date = date(today.year,today.month,today.day)  # set current date in format of YYYY-MM-DD\n",
    "    \n",
    "        dt = cur_date - start_date #calculate interval in days to use for loop\n",
    "        ##make directory to downloaded files\n",
    "        try:\n",
    "            os.makedirs(os.getcwd()+'/parliament_practicals') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        ##loop throught the interval with 1 day step and append the date to url along with categories of either House of Lords or Commons\n",
    "        for ul in url:\n",
    "        #    print ('Downloading: ',str('House of '+ul[112:-2]+'s'))\n",
    "            for i in tnrange(dt.days + 1,desc='Downloading: '+str(ul[112:-2]+'s')):\n",
    "                try: #test for errors and pass since there are dates that the House of Lords do not convene and HTTP request returns error; Also store results on folder parliament practicals\n",
    "                    filename = wget.download(ul+str(start_date + timedelta(days=i))+'.pdf',os.getcwd()+'/parliament_practicals')\n",
    "                except:\n",
    "                    next  \n",
    "    \n",
    "# Function to convert the downloaded pdfs to text files\n",
    "def convert_pdf_to_text(trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory textfiles already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/textfiles') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        list_of_files=os.listdir(os.getcwd()+'/parliament_practicals') # create a list of pdf files to be converted\n",
    "        for i in tnrange(len(list_of_files),desc='Converting pdf to txt'): # iterate throught the files on the list and install progress bar\n",
    "            if list_of_files[i].endswith(\".pdf\"): # check that file input is pdf file\n",
    "                parsedPDF=parser.from_file(os.getcwd()+'/parliament_practicals/'+list_of_files[i]) # parse pdf file\n",
    "                text_file = open(os.getcwd()+'/textfiles/'+list_of_files[i][:-4]+'.txt', 'a') # create new filename with extension .txt\n",
    "                text_file.write(parsedPDF[\"content\"]) # write parsed pdf to text\n",
    "                text_file.close() # close text file\n",
    "            else: # if file other than pdf continue loop\n",
    "                next\n",
    "def download_practicals_convert(start_date,trg):\n",
    "    page=\"http://www.parliament.uk/business/publications/written-questions-answers-statements/daily-reports/\" # set link to parliament daily questions and answers reports\n",
    "    data_control(page,start_date,trg) # call set data function\n",
    "    convert_pdf_to_text(trg)# convert to pdf function\n",
    "    print ('==Downloading and conversion to text files completed==')\n",
    "    \n",
    "# Call function to either download the data or set current folder as working folder...please make sure that\n",
    "# if data are give then those should be stored on the folder: 'parliament_practicals'\n",
    "\n",
    "download_practicals_convert(start_date,trg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Procedures for scraping, downloading, and converting exchange rates to exchange-rate shifts and storing in a data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define if readily available exchange-rate data will be used (trg= 'yes') or if they will be scraped (trg ='no')\n",
    "trg='yes' # change this value to yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Data were given====\n",
      "         Date    Rate\n",
      "0  1999-01-04 -0.0019\n",
      "1  1999-01-05  0.0082\n",
      "2  1999-01-06  0.0007\n",
      "3  1999-01-07  0.0068\n",
      "4  1999-01-08  0.0011\n"
     ]
    }
   ],
   "source": [
    "# Clean exchange data download function and transform to pandas dataframe\n",
    "\n",
    "def clean_ex(file_path):\n",
    "    data = pd.read_csv(file_path,sep=\" t \",header=None, encoding=\"ISO-8859-1\") # load the text file\n",
    "    remove_words=['Bank of England Statistical Interactive Database','Series 1 to 1','Spot exchange rate, Euro into Sterling','XUDLERS','Ã‚','Â']\n",
    "    for word in remove_words: # remove words\n",
    "        data=data.replace(word,np.nan) # remove words\n",
    "    data=data.dropna()# drop nan\n",
    "    data.reset_index() # reset indices\n",
    "    rate=data.iloc[::2] # extract odd rows\n",
    "    date=data.iloc[1::2] # extract even rows\n",
    "    date.reset_index(inplace=True,drop=True) # reset indices\n",
    "    rate.reset_index(inplace=True,drop=True) # reset indices\n",
    "    x=pd.concat([date,rate],axis=1) # concatenate date and rates\n",
    "    x.columns=['Rate','Date'] # rename columns\n",
    "    x=x.dropna()# drop na\n",
    "    x['Date'] = pd.to_datetime(x['Date']) # convert date column to date\n",
    "    x[['Rate']] = x[['Rate']].apply(pd.to_numeric) #convert exchange rate to float\n",
    "    x.dtypes #check data types\n",
    "    x = x.set_index('Date').diff() # calculate [rate(t+1) - rate(t)]\n",
    "    x.reset_index(inplace=True)# reset Date column\n",
    "    x['Date']=x['Date'].dt.strftime('%Y-%m-%d')# convert to string for join matching operations\n",
    "    x.Rate = x.Rate.shift(-1)# shift rate column by one day to account for the delay of the parliament report\n",
    "    x=x.dropna()# drop na\n",
    "    x=x.drop_duplicates('Date') # drop duplicates\n",
    "    x.to_csv(os.getcwd()+'/xr/exchangeRates_diff.csv')# save to csv file\n",
    "    return x # return dataframe\n",
    "\n",
    "# Define function to download exchange rates to text file in folder xr\n",
    "def download_xr(trg):\n",
    "    html= urllib.request.urlopen(\"http://www.bankofengland.co.uk/boeapps/iadb/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1963&TD=11&TM=Apr&TY=2017&VFD=Y&CSVF=TT&C=C8J&Filter=N&html.x=11&html.y=9\")\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "    else:\n",
    "        try: # test if directory xr already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/xr') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        soup_xr = BeautifulSoup(html)\n",
    "        xr = soup_xr.get_text()\n",
    "        #print(xr)\n",
    "        text_xr = open(os.getcwd()+'/xr/'+'exchangeRates'+'.txt', \"a\")\n",
    "        text_xr.write(xr)\n",
    "        text_xr.close()\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "        next\n",
    "    return data\n",
    "\n",
    "data_xr = download_xr(trg)\n",
    "print(data_xr.head()) # print head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 || Defining machine-learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Collate analysis dataset, run initial load, and perform transformations [Task A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of filename - text with numbers and punctuation removed\n",
    "\n",
    "# Set stop word parameters-for stopwords removal the stop_words pachage was used\n",
    "# The  StopWordsRemover from pyspark.ml.features was also tested extensively but was buggy / not effective\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "def remove_n_p(text): # function that removes punctuation and numbers as well lowercasing the text\n",
    "    text = re.sub(r'\\d+','', text) # remove numbers from texts with regular expressions\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\W+', ' ', text)# remove punctuation from texts with regular expressions\n",
    "    text=text.lower() # lowercase the text\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # remove stopwords\n",
    "    return text\n",
    "\n",
    "# Extract date from filename function \n",
    "def trim_filename(filename):\n",
    "    date=filename[-14:-4] # extract the timestamp from end of the file\n",
    "    return date # return date\n",
    "    \n",
    "# SparkSession added for spark dataframes\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def make_dataFrame(dirPath): # make a dataFrame with filename and text \n",
    "    ft_RDD = sc.wholeTextFiles(dirPath) # add code to create an RDD with wholeTextFiles\n",
    "    spm_t_RDD = ft_RDD.map(lambda ft: (trim_filename(ft[0]), remove_n_p(ft[1]))) # create RDD with filename and call remove_n_p function to text\n",
    "    file_text_df = spark.createDataFrame(spm_t_RDD,schema=['id','text']) # create a dataFrame - filename - text\n",
    "    return file_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|        id|                text|  label|\n",
      "+----------+--------------------+-------+\n",
      "|2017-02-24|daily report frid...|-0.0069|\n",
      "|2017-02-24|friday february p...|-0.0069|\n",
      "|2016-07-06|daily report wedn...| 0.0041|\n",
      "|2016-07-06|wednesday july p ...| 0.0041|\n",
      "|2017-03-08|daily report wedn...|-0.0044|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert currency pandas dataframe to Spark dataframe and specify datatypes\n",
    "\n",
    "def currency_df(data_xr):# function to create dataframe\n",
    "    data_xr_DF=spark.createDataFrame(data_xr,schema=['Date','Rate']) # create dataframe\n",
    "    data_xr_DF=data_xr_DF.withColumn('Rate', data_xr_DF['Rate'].cast('float'))# convert rate to float\n",
    "    return data_xr_DF # return spark dataframe\n",
    "\n",
    "# function to join the two dataframes on dates. Exchange rate dates are shifted back by one day\n",
    "# in order to account for the delay of the parliament publication\n",
    "def connect_xr_df(file_text_df,data_xr_DF): # the function takes the two inpurts of file_text_df and the data_xr_DF from the previous function\n",
    "    file_text_Date_rate=file_text_df.join(data_xr_DF,file_text_df.id==data_xr_DF.Date,'left_outer') # the exchange rates were connected to the timestamp of the parliament files with a matching left outer join\n",
    "    file_text_Date_rate.createOrReplaceTempView(\"temp\") # create a temporary sql view\n",
    "    file_text_Date_rate_sql = spark.sql(\"SELECT id,text,Rate as label FROM temp\") # select statement of the three columns required for analysis and relabeling\n",
    "    file_text_Date_rate_sql.show(5)\n",
    "    return file_text_Date_rate_sql # return the dataframe for analysis\n",
    "\n",
    "id_text_label=connect_xr_df(make_dataFrame(os.getcwd()+'/textfiles'),currency_df(data_xr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Implement a machine-learning pipeline in Spark, including feature extractors, transformers, and selectors [Task B] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "#Step2: make hashTF \n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "#Step 3: feed hash vector to calculate idf\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"idf\") \n",
    "\n",
    "#Step4: linear regression parameters\n",
    "lr_tf = LinearRegression()\\\n",
    "         .setFeaturesCol(\"features\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "lr_idf = LinearRegression()\\\n",
    "     .setFeaturesCol(\"idf\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "#Step 5: configure alternative pipelines \n",
    "pipeline_tf = Pipeline(stages=[tokenizer, hashingTF, lr_tf]) #with hash vector tf\n",
    "pipeline_idf = Pipeline(stages=[tokenizer, hashingTF, idf, lr_idf]) #with hash vector idf \n",
    "\n",
    "#Step 6: set exemplar parameter grid\n",
    "paramGrid_tf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000]) \\\n",
    "    .addGrid(lr_tf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_tf.maxIter, [50]) \\\n",
    "    .addGrid (lr_tf.elasticNetParam,[0.8])\\\n",
    "    .build()\n",
    "\n",
    "paramGrid_idf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000]) \\\n",
    "    .addGrid(lr_idf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_idf.maxIter, [50]) \\\n",
    "    .addGrid (lr_idf.elasticNetParam,[0.8])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate pipeline performance using training and test datasets [Task C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Train-Validation\n",
      "finished Train-Validation\n",
      "PipelineModel_401398cbedea7e2a888d\n",
      "---Linear Regression with hash TF predictors: R Squared for training dataset is 1.1102230246251565e-16 ---\n",
      "PipelineModel_401398cbedea7e2a888d\n",
      "---Linear Regression with hash TF predictors: R Squared for testing dataset is -0.05000512356683151 ---\n",
      "starting Train-Validation\n",
      "finished Train-Validation\n",
      "PipelineModel_463cab1342fb343af0bc\n",
      "---Linear Regression with hash IDF predictors: R Squared for training dataset is 1.1102230246251565e-16 ---\n",
      "PipelineModel_463cab1342fb343af0bc\n",
      "---Linear Regression with hash IDF predictors: R Squared for testing dataset is -0.05000512356683129 ---\n"
     ]
    }
   ],
   "source": [
    "[id_text_label_train, id_text_label_test] = id_text_label.randomSplit([0.8, 0.2], 25) # split id_text_label into training (80%) and testing (20%) subsets, seed = 25\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # Use R squared to evaluate performance of models (% of variance in xr shifts explained by predictors)\n",
    "\n",
    "### Modeling with pipeline_tf\n",
    "tvs_tf = TrainValidationSplit(estimator=pipeline_tf, \n",
    "                           estimatorParamMaps=paramGrid_tf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "# Run TrainValidationSplit on training dataset\n",
    "print('starting Train-Validation')\n",
    "tvsModel_tf = tvs_tf.fit(id_text_label_train)\n",
    "print('finished Train-Validation')\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_tf.transform(id_text_label_train)\n",
    "print(tvsModel_tf.bestModel)\n",
    "rsquared_tf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash TF predictors: R Squared for training dataset is %s ---\" % (rsquared_tf))\n",
    "\n",
    "# R squared for prediction on testing dataframe \n",
    "prediction  = tvsModel_tf.transform(id_text_label_test)\n",
    "print(tvsModel_tf.bestModel)\n",
    "rsquared_tf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash TF predictors: R Squared for testing dataset is %s ---\" % (rsquared_tf))\n",
    "\n",
    "### Modeling with pipeline pipeline_idf\n",
    "\n",
    "tvs_idf = TrainValidationSplit(estimator=pipeline_idf, \n",
    "                           estimatorParamMaps=paramGrid_idf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "print('starting Train-Validation')\n",
    "tvsModel_idf = tvs_idf.fit(id_text_label_train)\n",
    "print('finished Train-Validation')\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_idf.transform(id_text_label_train)\n",
    "print(tvsModel_idf.bestModel)\n",
    "rsquared_idf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash IDF predictors: R Squared for training dataset is %s ---\" % (rsquared_idf))\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_idf.transform(id_text_label_test)\n",
    "print(tvsModel_idf.bestModel)\n",
    "rsquared_idf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash IDF predictors: R Squared for testing dataset is %s ---\" % (rsquared_idf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Experiment with and evaluate alternative pipelines using grid [Task D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Train-Validation with training to validation ratio = (0.7)\n",
      "finished Train-Validation\n",
      "PipelineModel_4aed8d38e362b986c773\n",
      "---Linear Regression with hash TF predictors: R Squared for training dataset is 0.0 ---\n",
      "PipelineModel_4aed8d38e362b986c773\n",
      "---Linear Regression with hash TF predictors: R Squared for testing dataset is -0.05000512356683151 ---\n",
      "starting Train-Validation with training to validation ratio = (0.8)\n",
      "finished Train-Validation\n",
      "PipelineModel_441794d9653cd168b660\n",
      "---Linear Regression with hash TF predictors: R Squared for training dataset is -2.220446049250313e-16 ---\n",
      "PipelineModel_441794d9653cd168b660\n",
      "---Linear Regression with hash TF predictors: R Squared for testing dataset is -0.05000512356683129 ---\n",
      "starting Train-Validation with training to validation ratio = (0.7)\n",
      "finished Train-Validation\n",
      "PipelineModel_4a42b7461d273edff98e\n",
      "---Linear Regression with hash IDF predictors: R Squared for training dataset is 0.0 ---\n",
      "PipelineModel_4a42b7461d273edff98e\n",
      "---Linear Regression with hash IDF predictors: R Squared for testing dataset is -0.05000512356683107 ---\n",
      "starting Train-Validation with training to validation ratio = (0.8)\n",
      "finished Train-Validation\n",
      "PipelineModel_425386edc6e7431b4ba7\n",
      "---Linear Regression with hash IDF predictors: R Squared for training dataset is 0.0 ---\n",
      "PipelineModel_425386edc6e7431b4ba7\n",
      "---Linear Regression with hash IDF predictors: R Squared for testing dataset is -0.05000512356683129 ---\n"
     ]
    }
   ],
   "source": [
    "#Set experiment parameter grid\n",
    "paramGrid_tf_exp = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000, 200000]) \\\n",
    "    .addGrid(lr_tf.regParam, [0.1, 0.3]) \\\n",
    "    .addGrid(lr_tf.maxIter, [20, 50]) \\\n",
    "    .addGrid (lr_tf.elasticNetParam,[0.1, 0.8])\\\n",
    "    .build()\n",
    "\n",
    "paramGrid_idf_exp = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000, 200000]) \\\n",
    "    .addGrid(lr_idf.regParam, [0.1, 0.3]) \\\n",
    "    .addGrid(lr_idf.maxIter, [20, 50]) \\\n",
    "    .addGrid (lr_idf.elasticNetParam,[0.1, 0.8])\\\n",
    "    .build()\n",
    "\n",
    "[id_text_label_train, id_text_label_test] = id_text_label.randomSplit([0.8, 0.2], 25) # split id_text_label into training (80%) and testing (20%) subsets, seed = 25\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # Use R squared to evaluate performance of models (% of variance in xr shifts explained by predictors)\n",
    "\n",
    "### Experiments with pipeline_tf\n",
    "\n",
    "for ratio in [0.7, 0.8]:\n",
    "    tvs_tf_exp = TrainValidationSplit(estimator=pipeline_tf,\n",
    "                           estimatorParamMaps=paramGrid_tf_exp,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=ratio) # 80% of the data will be used for training, 20% for validation\n",
    "    # Run TrainValidationSplit on training dataset\n",
    "    print('starting Train-Validation with training to validation ratio = ('+str(ratio)+')')\n",
    "    tvsModel_tf_exp = tvs_tf_exp.fit(id_text_label_train)\n",
    "    print('finished Train-Validation')\n",
    "    # R squared for prediction on training dataframe \n",
    "    prediction  = tvsModel_tf_exp.transform(id_text_label_train)\n",
    "    print(tvsModel_tf_exp.bestModel)\n",
    "    rsquared_tf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash TF predictors: R Squared for training dataset is %s ---\" % (rsquared_tf_exp))\n",
    "    # R squared for prediction on testing dataframe \n",
    "    prediction  = tvsModel_tf_exp.transform(id_text_label_test)\n",
    "    print(tvsModel_tf_exp.bestModel)\n",
    "    rsquared_tf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash TF predictors: R Squared for testing dataset is %s ---\" % (rsquared_tf_exp))\n",
    "\n",
    "### Experiments with pipeline pipeline_idf\n",
    "\n",
    "for ratio in [0.7, 0.8]:\n",
    "    tvs_idf_exp = TrainValidationSplit(estimator=pipeline_idf,\n",
    "                           estimatorParamMaps=paramGrid_idf_exp,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=ratio) # 80% of the data will be used for training, 20% for validation\n",
    "    # Run TrainValidationSplit on training dataset\n",
    "    print('starting Train-Validation with training to validation ratio = ('+str(ratio)+')')\n",
    "    tvsModel_idf_exp = tvs_idf_exp.fit(id_text_label_train)\n",
    "    print('finished Train-Validation')\n",
    "    # R squared for prediction on training dataframe \n",
    "    prediction  = tvsModel_idf_exp.transform(id_text_label_train)\n",
    "    print(tvsModel_idf_exp.bestModel)\n",
    "    rsquared_idf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash IDF predictors: R Squared for training dataset is %s ---\" % (rsquared_idf_exp))\n",
    "    # R squared for prediction on testing dataframe \n",
    "    prediction  = tvsModel_idf_exp.transform(id_text_label_test)\n",
    "    print(tvsModel_idf_exp.bestModel)\n",
    "    rsquared_idf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash IDF predictors: R Squared for testing dataset is %s ---\" % (rsquared_idf_exp))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 || Comments / summary of findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this preliminary analysis suggest that the content of political speech at the House of Commons and the House of Lords does not account for the variation in shifts of GBP-EUR exchange rates, at least to a degree that is detectible by the linear regression models that were trained, validated, and tested in this study. The proportion of variation in exchange-rate shifts predicted by either term frequencies or inverse document frequencies (as indicated by R squared metrics presented in this notebook) at the training stage of the linear regression algorithm is c.0 for alternative linear regression parameters. The experiments conducted suggest that increasing the number of hashes (thus reducing collisions during text processing) does not improve results. R squared metrics yielded at the testing phase of the algorithms are systematically marginally below 0. This is indicative of poor predictions on the testing datasets; poorer than predictions at random. Overall, findings are probably non-exciting but certainly non counter-intuitive, since the outcome we attempted to predict is complex, dynamic, and dependent upon highly complex market processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
