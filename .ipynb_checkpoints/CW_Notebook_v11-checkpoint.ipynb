{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432: Big Data - Coursework (Part II)\n",
    "\n",
    "## Using the content of post-EU-referendum Great British parliamentary debates to predict GBP-EUR exchange rates: A pySpark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alexandros Dimitrios Nalmpantis; Georgios Kyriakopoulos (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Abstract_** This notebook presents pySpark code that implements a machine-learning pipeline, which (a) scrapes and processes daily reports on debates at the House of Commons and the House of Lords from 23 June 2016 onwards; (b) scrapes daily GBP-EUR exchange rates and calculates their lag; and (c) links the two to train a regression-based algorithm that predicts the latter from the former. Lags of exchange rates are being used (instead of raw exchange rates) to address issues of autocorrelation in the modelling process. Alternative model hyperparameters are assessed at a grid-search approach, which involves training, validating, and testing model performance.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Modules needed for the analysis are imported below.\n",
    "* Some modules may need to be installed with the following commands to a termninal:\n",
    "    **pip install <\"name of module\">** eg: pip install tqdm\n",
    "    or **with conda install <\"name of module\">** eg: conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import modules for scraping links\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date,timedelta\n",
    "import os\n",
    "\n",
    "# Import modules for downloading links\n",
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "# Import midules for parsing pdf's,progress bars and handling errors\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Import modules for spark ML, math and operators\n",
    "import re\n",
    "from operator import add\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark import SparkContext as sc\n",
    "from math import log\n",
    "import time\n",
    "from pprint import pprint\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###     2.1 Wrapped procedures of scraping, downloading and converting parliamentary debates to text files\n",
    "\n",
    "Daily reports of debates at the House of Lords and the House of Commons are published on the parliament website as PDFs. Below we scrape and download these, before converting them into txt format. Using the trigger argument *trg* we offer the option of not downloading the reports, if these are already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data control function that controls wether the data will be scraped or were provided by students\n",
    "def data_control(page,start_date,trg):\n",
    "    if trg=='yes':\n",
    "        os.chdir(os.getcwd)\n",
    "    else:\n",
    "        html_page = urllib.request.urlopen(page) #request page with urllib packages\n",
    "        soup = BeautifulSoup(html_page) #pass the page to beautiful soup in order to extract the links contained in webpage \n",
    "        #print (soup) #visually inspect the html structure\n",
    "        hl = [] #set hyperlink array to store the extracted links\n",
    "        ##search html for hyperlinks starting with qna\n",
    "        for hyperlink in soup.findAll('a', attrs={'href': re.compile(\"^http://qna\")}): \n",
    "            hl.append(hyperlink.get('href')) #store the hyperlinks found on an array\n",
    "        #    print (link.get('href'))\n",
    "        \n",
    "        url=[hl[1][:-20]+'Lords-',hl[1][:-20]+'Commons-'] #take first result and cut the dates and category of either lords or commons\n",
    "    \n",
    "        #date interval search set and downloading of the pdf files\n",
    "        ##create interval search date\n",
    "        today=datetime.datetime.today() #today's date set\n",
    "        cur_date = date(today.year,today.month,today.day)  # set current date in format of YYYY-MM-DD\n",
    "    \n",
    "        dt = cur_date - start_date #calculate interval in days to use for loop\n",
    "        #make directory to downloaded files\n",
    "        try:\n",
    "            os.makedirs(os.getcwd()+'/parliament_practicals') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        #loop throught the interval with 1 day step and append the date to url along with categories of either house of Lords or Commons\n",
    "        for ul in url:\n",
    "        #    print ('Downloading: ',str('House of '+ul[112:-2]+'s'))\n",
    "            for i in tnrange(dt.days + 1,desc='Downloading: '+str(ul[112:-2]+'s')):\n",
    "                try: #test for errors and pass since there are dates that the House of Lords do not convene and HTTP request returns error; Also store results on folder parliament practicals\n",
    "                    filename = wget.download(ul+str(start_date + timedelta(days=i))+'.pdf',os.getcwd()+'/parliament_practicals')\n",
    "                except:\n",
    "                    next  \n",
    "    \n",
    "# function to convert the downloaded pdfs to text files\n",
    "def convert_pdf_to_text(trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory textfiles already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/textfiles') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        list_of_files=os.listdir(os.getcwd()+'/parliament_practicals') # create a list of pdf files to be converted\n",
    "        for i in tnrange(len(list_of_files),desc='Converting pdf to txt'): # iterate throught the files on the list and install progress bar\n",
    "            if list_of_files[i].endswith(\".pdf\"): # check that file input is pdf file\n",
    "                parsedPDF=parser.from_file(os.getcwd()+'/parliament_practicals/'+list_of_files[i]) # parse pdf file\n",
    "                text_file = open(os.getcwd()+'/textfiles/'+list_of_files[i][:-4]+'.txt', 'a') # create new filename with extension .txt\n",
    "                text_file.write(parsedPDF[\"content\"]) # write parsed pdf to text\n",
    "                text_file.close() # close text file\n",
    "            else: # if file other than pdf continue loop\n",
    "                next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###     2.2 User input of parameters and function calling\n",
    "Here, call the functions that scrape, download, and produce daily parliamentary-debate reports in txt format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# If data are given for time saving purposes then set the following parameter to yes\n",
    "trg='no'\n",
    "# set link to parliament daily questions and answers reports\n",
    "page=\"http://www.parliament.uk/business/publications/written-questions-answers-statements/daily-reports/\" # set link to parliament daily questions and answers reports\n",
    "# set from which date to current date the function will download reports in YYYY-M-D format below\n",
    "start_date = date(2016, 6, 23)\n",
    "\n",
    "# Call function to either download the data or set current folder as working folder...please make sure that\n",
    "# if data are give then those should be stored on the folder: 'parliament_practicals'\n",
    "# We suggest to run the scraping function since it only takes 2minutes for downloading a year of reports\n",
    "data_control(page,start_date,trg) # call set data function\n",
    "convert_pdf_to_text(trg) # convert to pdf function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 Defining function for sourcing daily exchenge rates \n",
    "Daily GBP-EUR exchange rates are published by the Bank of England. Below, we download these in a txt format. Again, using the trigger argument *trg* we offer the option of not downloading the exchange rates, if these are already available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define function to download exchange rates to text file in folder xr\n",
    "def download_xr(html, trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory xr already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/xr') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        soup_xr = BeautifulSoup(html)\n",
    "        xr = soup_xr.get_text()\n",
    "        #print(xr)\n",
    "        text_xr = open(os.getcwd()+'/xr/'+'exchangeRates'+'.txt', \"a\")\n",
    "        text_xr.write(xr)\n",
    "        text_xr.close()\n",
    "        next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 User input of parameters and function calling\n",
    "Here, we call the function that scrapes and downloads daily GBP-EUR exchange rates in txt format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trg='no'\n",
    "html_page_xr = urllib.request.urlopen(\"http://www.bankofengland.co.uk/boeapps/iadb/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1963&TD=11&TM=Apr&TY=2017&VFD=Y&CSVF=TT&C=C8J&Filter=N&html.x=11&html.y=9\")\n",
    "download_xr(html_page_xr, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean exchange data download function and transform to pandas dataframe\n",
    "def clean_ex(file_path):\n",
    "    data = pd.read_csv(file_path,sep=\" t \",header=None, encoding=\"ISO-8859-1\") # load the text file\n",
    "    data = data.loc[6:, 0:2] # remove headers\n",
    "    data.reset_index(inplace=True,drop=True) # reset indexes\n",
    "    date=data.iloc[::2] # extract odd rows\n",
    "    rate=data.iloc[1::2] # extract even columns\n",
    "    date.reset_index(inplace=True,drop=True) # reset indexes\n",
    "    rate.reset_index(inplace=True,drop=True) # reset indexes\n",
    "    x=pd.concat([date,rate],axis=1) # concatenate date and rates\n",
    "    x.columns=['Date','Rate'] # rename columns\n",
    "    return x # return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\analmpantis\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date    Rate\n",
      "0  05 Jan 99  1.4048\n",
      "1  06 Jan 99   1.413\n",
      "2  07 Jan 99  1.4137\n",
      "3  08 Jan 99  1.4205\n",
      "4  11 Jan 99  1.4216\n"
     ]
    }
   ],
   "source": [
    "data = clean_ex(os.getcwd()+'/xr/exchangeRates.txt') # call clean function\n",
    "print(data.head()) # print head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3 Load spark and read and split files to f,w tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "wholeTextFiles() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4fd01a002160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile_word_RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfile_word_RDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file_word_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/textfiles'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# apply function (b) on the text corpus for the analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_word_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print (file, word) tuples indicatively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4fd01a002160>\u001b[0m in \u001b[0;36mread_file_word_RDD\u001b[0;34m(argDir)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file_word_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# function (b) builds (file, word) tuples using function (a) (which builds (file, word) tuples from (file, text) tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfile_text_RDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwholeTextFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# read the files and build (file, text) tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfile_word_RDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_text_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitFileWords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use function (a)to build (file, word) tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print('Read {} files from directory {}'.format(file_text_RDD.count(), argDir)) # print count and location of files used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: wholeTextFiles() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "def splitFileWords(file_text): # function (a) builds (file, word) tuples from (file, text) tuples\n",
    "    f,t = file_text # define the input to the function\n",
    "    file_word_List = [] # create an empty (file,word) list\n",
    "    word_List = re.split('\\W+',t) # split texts into words using regular expression\n",
    "    for w in word_List: \n",
    "        file_word_List.append((f,w.lower())) # append words in lowercase to their corresponding file\n",
    "    return file_word_List\n",
    "\n",
    "def read_file_word_RDD(argDir): # function (b) builds (file, word) tuples using function (a) (which builds (file, word) tuples from (file, text) tuples \n",
    "    file_text_RDD = sc.wholeTextFiles(argDir)# read the files and build (file, text) tuples\n",
    "    file_word_RDD = file_text_RDD.flatMap(splitFileWords) #use function (a)to build (file, word) tuples\n",
    "    #print('Read {} files from directory {}'.format(file_text_RDD.count(), argDir)) # print count and location of files used\n",
    "    #print('file word count histogram')\n",
    "    #print(file_word_RDD.map(lambda fwL: (len(fwL[1]))).histogram([0,10,100,500, 1000, 5000, 10000])) # print word-count histogram \n",
    "    return file_word_RDD \n",
    "\n",
    "file_word_RDD = read_file_word_RDD(os.getcwd()+'/textfiles') # apply function (b) on the text corpus for the analysis \n",
    "pprint(file_word_RDD.take(2)) # print (file, word) tuples indicatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
