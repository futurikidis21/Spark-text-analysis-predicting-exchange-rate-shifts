{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432: Big Data - Coursework (Part II)\n",
    "\n",
    "## Predicting shifts in GBP-EUR exchange rates based on the content of UK parliamentary debates: A pySpark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alexandros Dimitrios Nalmpantis; Georgios Kyriakopoulos (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The pySpark code presented in this notebook aims to construct a modelling process, which predicts shifts in GBP-EUR exchange rates on the basis of political narrative. More specifically, the code implements a process that, schematically speaking, involves the following steps:\n",
    "\n",
    "(a) It scrapes the (almost daily) reports that record debates at the House of Commons and the House of Lords, as published on the UK Government website. The reports (available in PDF format) are converted into txt format, before removing common and usually uninformative words (i.e. stopwords), numeric figures, and punctuation symbols. File names and the remainder text content are converted into a data-frame. Following the construction of this dataframe, the content of the debate reports is tonekinsed and hashed, while term frequencies (TFs) and inverse document frequencies (IDFs) are calculated for the hashes.\n",
    "\n",
    "(b) It scrapes the (almost daily) EUR-GBP exchange rates, published on the Bank of England website. The content is written in a text file, which is then processed to remove blank space and to calculate the difference between each GBP-EUR exchange-rate value and its immediately previous exchange-rate value (i.e. *the exchange-rate shift*). Dates and exhange-rate shifts are then converted into a second data-frame.\n",
    "\n",
    "(c) It links the data frames constracted at (a) and (b) together, so that the content of the debate reports at a certain date is appended to the shift in GBP-EUR exchange rates recorded the day after. This new dataframe serves as the analytical input on the basis of which the main analysis is conducted.\n",
    "\n",
    "(d) Finally, it constructs  a machine learning pipeline, whereby the linear regression algorithm is trained (and validated) to predict exchange-rate shifts based on either the TF or the IDF of the (tokenised and hashed) content of the debate reports. Alternative parameterisation options are explored using a grid, to help optimise prediction.\n",
    "\n",
    "The findings of the analysis are highlighted within the anotation of this code and, broadly speaking, they appear to suggest that data-driven approaches (such as the one presented here) have the potential to detect statistical links  between  macroeconomic finacial-market parameters and topical political narratives ( *UPDATE ACCORDINGLY*). It is acknowledged that time-series modelling techniques (rather than linear regression) would be a more appropriate analytical approach, given the nature of the analysis dataset. However, for the purposes of this preliminary study we are (partially) adressing the issue of autocorrelations between data points by focusing the analysis on exchange-rate shifts rather than exchange rate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Modules needed for the analysis are imported below. Some modules may need to be installed with the following commands to a termninal: **pip install <\"name of module\">** eg: pip install tqdm  or **with conda install <\"name of module\">** eg: conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import modules for scraping links\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date,timedelta\n",
    "import os\n",
    "\n",
    "# Import modules for downloading links\n",
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import modules for parsing pdf's,progress bars and handling errors\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Import modules for spark ML and SQL\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "# import various operators\n",
    "from math import log\n",
    "import time\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from  stop_words import get_stop_words\n",
    "\n",
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set sparkcontext as sc\n",
    "sc=SparkContext() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Data collection and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We note that the process of data collection (implemented in this section) depends on the format and structure at which  the data scraped are published on the UK Government and the Bank of ENgland websites. \n",
    "The functionality of the code was confirmed on 23/04/2017. Future changes at the sources of the data may affect the functionality of the code; we, therefore, provide the datafiles scraped on 23/04/2017, so that the analysis can be repeated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###     2.1 Defining and calling wrapped procedures from scraping, downloading and converting parliamentary debates to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define if report data were provided ( yes - data were provided, no - data need to be scraped)\n",
    "trg='no'\n",
    "\n",
    "    # set from which date to current date that the function will download reports in YYYY-M-D format below\n",
    "start_date = date(2016, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6528b318112e49d1976affa085bee0af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dff69c29fe4fb1ada60a1835b94c80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98452ea57fb43638e81bc8d3a454977"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-24 19:22:50,688 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.14/tika-server-1.14.jar to /var/folders/0c/dkmpfbdd6h96whwytkxqnp880000gp/T/tika-server.jar.\n",
      "2017-04-24 19:23:05,349 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.14/tika-server-1.14.jar.md5 to /var/folders/0c/dkmpfbdd6h96whwytkxqnp880000gp/T/tika-server.jar.md5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==Downloading and conversion to text files completed==\n"
     ]
    }
   ],
   "source": [
    "# Data control function that controls wether the data will be scraped or were provided by students\n",
    "def data_control(page,start_date,trg):\n",
    "    if trg=='yes':\n",
    "        os.chdir(os.getcwd)\n",
    "    else:\n",
    "        html_page = urllib.request.urlopen(page) #request page with urllib packages\n",
    "        soup = BeautifulSoup(html_page) #pass the page to beautiful soup in order to extract the links contained in webpage \n",
    "        #print (soup) #visually inspect the html structure\n",
    "        hl = [] #set hyperlink array to store the extracted links\n",
    "        ##search html for hyperlinks starting with qna\n",
    "        for hyperlink in soup.findAll('a', attrs={'href': re.compile(\"^http://qna\")}): \n",
    "            hl.append(hyperlink.get('href')) #store the hyperlinks found on an array\n",
    "        #    print (link.get('href'))\n",
    "        \n",
    "        url=[hl[1][:-22]+'Lords-',hl[1][:-22]+'Commons-'] #take first result and cut the dates and category of either lords or commons\n",
    "    \n",
    "        #date interval search set and downloading of the pdf files\n",
    "        ##create interval search date\n",
    "        today=datetime.datetime.today() #today's date set\n",
    "        cur_date = date(today.year,today.month,today.day)  # set current date in format of YYYY-MM-DD\n",
    "    \n",
    "        dt = cur_date - start_date #calculate interval in days to use for loop\n",
    "        #make directory to downloaded files\n",
    "        try:\n",
    "            os.makedirs(os.getcwd()+'/parliament_practicals') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        #loop throught the interval with 1 day step and append the date to url along with categories of either house of Lords or Commons\n",
    "        for ul in url:\n",
    "        #    print ('Downloading: ',str('House of '+ul[112:-2]+'s'))\n",
    "            for i in tnrange(dt.days + 1,desc='Downloading: '+str(ul[112:-2]+'s')):\n",
    "                try: #test for errors and pass since there are dates that the House of Lords do not convene and HTTP request returns error; Also store results on folder parliament practicals\n",
    "                    filename = wget.download(ul+str(start_date + timedelta(days=i))+'.pdf',os.getcwd()+'/parliament_practicals')\n",
    "                except:\n",
    "                    next  \n",
    "    \n",
    "# function to convert the downloaded pdfs to text files\n",
    "def convert_pdf_to_text(trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory textfiles already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/textfiles') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        list_of_files=os.listdir(os.getcwd()+'/parliament_practicals') # create a list of pdf files to be converted\n",
    "        for i in tnrange(len(list_of_files),desc='Converting pdf to txt'): # iterate throught the files on the list and install progress bar\n",
    "            if list_of_files[i].endswith(\".pdf\"): # check that file input is pdf file\n",
    "                parsedPDF=parser.from_file(os.getcwd()+'/parliament_practicals/'+list_of_files[i]) # parse pdf file\n",
    "                text_file = open(os.getcwd()+'/textfiles/'+list_of_files[i][:-4]+'.txt', 'a') # create new filename with extension .txt\n",
    "                text_file.write(parsedPDF[\"content\"]) # write parsed pdf to text\n",
    "                text_file.close() # close text file\n",
    "            else: # if file other than pdf continue loop\n",
    "                next\n",
    "def download_practicals_convert(start_date,trg):\n",
    "    page=\"http://www.parliament.uk/business/publications/written-questions-answers-statements/daily-reports/\" # set link to parliament daily questions and answers reports\n",
    "    data_control(page,start_date,trg) # call set data function\n",
    "    convert_pdf_to_text(trg)# convert to pdf function\n",
    "    print ('==Downloading and conversion to text files completed==')\n",
    "    \n",
    "    # Call function to either download the data or set current folder as working folder...please make sure that\n",
    "    # if data are give then those should be stored on the folder: 'parliament_practicals'\n",
    "    # We suggest to run the scraping function since it only takes 2minutes for downloading a year of reports   \n",
    "download_practicals_convert(start_date,trg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Defining and calling wrapped procedures for scraping, downloading and converting time-series exchange-rate shifts to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define if exchange rate data were provided ( yes - data were provided, no - data need to be scraped)\n",
    "trg='yes' # change this value to yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Data were given====\n",
      "         Date    Rate\n",
      "0  1999-01-04 -0.0019\n",
      "1  1999-01-05  0.0082\n",
      "2  1999-01-06  0.0007\n",
      "3  1999-01-07  0.0068\n",
      "4  1999-01-08  0.0011\n"
     ]
    }
   ],
   "source": [
    "# define function to download exchange rates to text file in folder xr\n",
    "# Clean exchange data download function and transform to pandas dataframe\n",
    "def clean_ex(file_path):\n",
    "    data = pd.read_csv(file_path,sep=\" t \",header=None, encoding=\"ISO-8859-1\") # load the text file\n",
    "    remove_words=['Bank of England Statistical Interactive Database','Series 1 to 1','Spot exchange rate, Euro into Sterling','XUDLERS','Ã‚','Â']\n",
    "    for word in remove_words: # remove words\n",
    "        data=data.replace(word,np.nan) # remove words\n",
    "    data=data.dropna()# drop nan\n",
    "    data.reset_index() # reset indices\n",
    "    rate=data.iloc[::2] # extract odd rows\n",
    "    date=data.iloc[1::2] # extract even rows\n",
    "    date.reset_index(inplace=True,drop=True) # reset indices\n",
    "    rate.reset_index(inplace=True,drop=True) # reset indices\n",
    "    x=pd.concat([date,rate],axis=1) # concatenate date and rates\n",
    "    x.columns=['Rate','Date'] # rename columns\n",
    "    x=x.dropna()# drop na\n",
    "    x['Date'] = pd.to_datetime(x['Date']) # convert date column to date\n",
    "    x[['Rate']] = x[['Rate']].apply(pd.to_numeric) #convert exchange rate to float\n",
    "    x.dtypes #check data types\n",
    "    x = x.set_index('Date').diff() # calculate [rate(t+1) - rate(t)]\n",
    "    x.reset_index(inplace=True)# reset Date column\n",
    "    x['Date']=x['Date'].dt.strftime('%Y-%m-%d')# convert to string for join matching operations\n",
    "    x.Rate = x.Rate.shift(-1)# shift rate column by one day to account for the delay of the parliament report\n",
    "    x=x.dropna()# drop na\n",
    "    x=x.drop_duplicates('Date') # drop duplicates\n",
    "    x.to_csv(os.getcwd()+'/xr/exchangeRates_diff.csv')# save to csv file\n",
    "    return x # return dataframe\n",
    "\n",
    "def download_xr(trg):\n",
    "    html= urllib.request.urlopen(\"http://www.bankofengland.co.uk/boeapps/iadb/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1963&TD=11&TM=Apr&TY=2017&VFD=Y&CSVF=TT&C=C8J&Filter=N&html.x=11&html.y=9\")\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "    else:\n",
    "        try: # test if directory xr already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/xr') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        soup_xr = BeautifulSoup(html)\n",
    "        xr = soup_xr.get_text()\n",
    "        #print(xr)\n",
    "        text_xr = open(os.getcwd()+'/xr/'+'exchangeRates'+'.txt', \"a\")\n",
    "        text_xr.write(xr)\n",
    "        text_xr.close()\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "        next\n",
    "    return data\n",
    "\n",
    "data_xr = download_xr(trg)\n",
    "print(data_xr.head()) # print head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task A: Select a dataset and make initial load and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of filename - text with numbers and punctuation removed\n",
    "\n",
    "# Set stop word parameters-for stopwords removal the stop_words pachage was used\n",
    "# The  StopWordsRemover from pyspark.ml.features was also tested extensively but was not effective or buggy\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "def remove_n_p(text): # function that removes punctuation and numbers as well lowercasing the text\n",
    "    text = re.sub(r'\\d+','', text) # remove numbers from texts with regular expressions <<<<<\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\W+', ' ', text)# remove punctuation from texts with regular expressions <<<<<\n",
    "    text=text.lower() # lowercase the text\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # remove stopwords\n",
    "    return text\n",
    "\n",
    "# extract date from filename function \n",
    "def trim_filename(filename):\n",
    "    date=filename[-14:-4] # extract the timestamp from end of the file\n",
    "    return date # return date\n",
    "    \n",
    "# sparksession added for spark dataframes\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def make_dataFrame(dirPath): # make a dataFrame with filename and text \n",
    "    ft_RDD = sc.wholeTextFiles(dirPath) # add code to create an RDD with wholeTextFiles\n",
    "    spm_t_RDD = ft_RDD.map(lambda ft: (trim_filename(ft[0]), remove_n_p(ft[1]))) # create RDD with filename and call remove_n_p function to text\n",
    "    file_text_df = spark.createDataFrame(spm_t_RDD,schema=['id','text']) # create a dataFrame - filename - text\n",
    "    return file_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert currency pandas dataframe to Spark Dataframe and specify datatypes\n",
    "\n",
    "def currency_df(data_xr):# function to create dataframe\n",
    "    data_xr_DF=spark.createDataFrame(data_xr,schema=['Date','Rate']) # create dataframe\n",
    "    data_xr_DF=data_xr_DF.withColumn('Rate', data_xr_DF['Rate'].cast('float'))# convert rate to float\n",
    "    return data_xr_DF # return spark dataframe\n",
    "\n",
    "# function to join the two dataframes on dates, the exchange rate dates have been shifted back by one day\n",
    "# in order to account for the delay of the parliament publication\n",
    "def connect_xr_df(file_text_df,data_xr_DF): # the function takes the two inpurts of file_text_df and the data_xr_DF from the previous function\n",
    "    file_text_Date_rate=file_text_df.join(data_xr_DF,file_text_df.id==data_xr_DF.Date,'left_outer') # the exchange rates were connected to the timestamp of the parliament files with a matching left outer join\n",
    "    file_text_Date_rate.createOrReplaceTempView(\"temp\") # create a temporary sql view\n",
    "    file_text_Date_rate_sql = spark.sql(\"SELECT id,text,Rate as label FROM temp\") # select statement of the three columns required for analysis and relabeling\n",
    "    file_text_Date_rate_sql.show(5)\n",
    "    return file_text_Date_rate_sql # return the dataframe for analysis\n",
    "\n",
    "#id_text_label=connect_xr_df(make_dataFrame(os.getcwd()+'/textfiles'),currency_df(data_xr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task B: Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "#Step2: make hashTF \n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "#Step 3: feed hash vector to calculate idf\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"idf\") \n",
    "\n",
    "#Step4: linear regression parameters\n",
    "lr_tf = LinearRegression()\\\n",
    "         .setFeaturesCol(\"features\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "lr_idf = LinearRegression()\\\n",
    "     .setFeaturesCol(\"idf\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "#Step 5: configure alternative pipelines \n",
    "pipeline_tf = Pipeline(stages=[tokenizer, hashingTF, lr_tf]) #with hash vector tf\n",
    "pipeline_idf = Pipeline(stages=[tokenizer, hashingTF, idf, lr_idf]) #with hash vector idf \n",
    "\n",
    "#Step 6: set exemplar parameter grid\n",
    "paramGrid_tf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000]) \\\n",
    "    .addGrid(lr_tf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_tf.maxIter, [50]) \\\n",
    "    .addGrid (lr_tf.elasticNetParam,[0.8])\\\n",
    "    .build()\n",
    "\n",
    "paramGrid_idf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000]) \\\n",
    "    .addGrid(lr_idf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_idf.maxIter, [50]) \\\n",
    "    .addGrid (lr_idf.elasticNetParam,[0.8])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_4081a08ecd728c36ce8a"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task C: Evaluate the performance of your pipeline using training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Train-Validation\n",
      "finished Train-Validation\n",
      "PipelineModel_4e5490e1e114820362cb\n",
      "---Linear Regression with hash TF predictors: R Squared for training dataset is -2.220446049250313e-16 ---\n",
      "PipelineModel_4e5490e1e114820362cb\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Nothing has been added to this summarizer.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o767.evaluate.\n: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.normL2(MultivariateOnlineSummarizer.scala:280)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.r2(RegressionMetrics.scala:125)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:88)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-52295558ad94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprediction\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtvsModel_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_text_label_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtvsModel_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrsquared_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---Linear Regression with hash TF predictors: R Squared for testing dataset is %s ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrsquared_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Nothing has been added to this summarizer.'"
     ]
    }
   ],
   "source": [
    "[id_text_label_train, id_text_label_test] = id_text_label.randomSplit([0.8, 0.2], 25) # split id_text_label into training (80%) and testing (20%) subsets, seed = 25\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # Use R squared to evaluate performance of models (% of variance in xr shifts explained by predictors)\n",
    "\n",
    "### Modeling with pipeline pipeline_tf\n",
    "\n",
    "tvs_tf = TrainValidationSplit(estimator=pipeline_tf, \n",
    "                           estimatorParamMaps=paramGrid_tf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "print('starting Train-Validation')\n",
    "tvsModel_tf = tvs_tf.fit(id_text_label_train)\n",
    "print('finished Train-Validation')\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_tf.transform(id_text_label_train)\n",
    "print(tvsModel_tf.bestModel)\n",
    "rsquared_tf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash TF predictors: R Squared for training dataset is %s ---\" % (rsquared_tf))\n",
    "\n",
    "# R squared for prediction on testing dataframe \n",
    "prediction  = tvsModel_tf.transform(id_text_label_test)\n",
    "print(tvsModel_tf.bestModel)\n",
    "rsquared_tf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash TF predictors: R Squared for testing dataset is %s ---\" % (rsquared_tf))\n",
    "\n",
    "### Modeling with pipeline pipeline_idf\n",
    "\n",
    "tvs_idf = TrainValidationSplit(estimator=pipeline_idf, \n",
    "                           estimatorParamMaps=paramGrid_idf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "print('starting Train-Validation')\n",
    "tvsModel_idf = tvs_idf.fit(id_text_label_train)\n",
    "print('finished Train-Validation')\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_idf.transform(id_text_label_train)\n",
    "print(tvsModel_idf.bestModel)\n",
    "rsquared_idf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash IDF predictors: R Squared for training dataset is %s ---\" % (rsquared_idf))\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_idf.transform(id_text_label_test)\n",
    "print(tvsModel_idf.bestModel)\n",
    "rsquared_idf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash IDF predictors: R Squared for testing dataset is %s ---\" % (rsquared_idf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task D: Implement a parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build parameters required for experiments\n",
    "\n",
    "# Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "#Step2: make hashTF sparse vector\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features') # set parameters for hashing\n",
    "    \n",
    "#Step 3: feed hash vector to calculate idf\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol='idf') # set parameters for idf\n",
    "def linear_regression(feature):\n",
    "    #Step4: linear regression parameters and if statement for idf\n",
    "    if feature=='hTF': # input for hashing only pipeline\n",
    "        lr = LinearRegression().setFeaturesCol('features').setLabelCol('label')\n",
    "    else: #input for hashing-idf pipeline\n",
    "        lr = LinearRegression().setFeaturesCol('idf').setLabelCol('label')\n",
    "    return lr\n",
    "\n",
    "# Pipeline function to fit the process of building a pipeline on various inputs\n",
    "def pipeline_fc(feature): # next step function for bulding pipeline\n",
    "    #Step5: set pipeline according to feature\n",
    "    if feature=='hTF': # input for hashing only pipeline\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF, linear_regression(feature)]) \n",
    "    else: #input for hashing-idf pipeline\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF,idf,linear_regression(feature)])\n",
    "    return pipeline # return pipeline\n",
    "\n",
    "#  Step 6: parameter grid set from user input on parameter dictionary\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # evaluator setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|        id|                text|  label|\n",
      "+----------+--------------------+-------+\n",
      "|2017-04-04|tuesday april p r...|  0.003|\n",
      "|2017-04-06|thursday april p ...|-0.0054|\n",
      "|2017-03-30|thursday march p ...| 0.0058|\n",
      "|2017-04-05|wednesday april p...| 0.0021|\n",
      "|2017-04-03|monday april p r ...|-0.0045|\n",
      "+----------+--------------------+-------+\n",
      "\n",
      "\n",
      "***** Experiment initiated *****\n",
      "\n",
      " Parameters: Train Ratio(0.6),Feature :(idf)\n",
      "Best model on training dataframe: PipelineModel_4b1ca3deb431353888f0\n",
      "---Linear Regression with hash IDF predictors: R Squared for training dataset is 1.0 ---\n",
      "Best model on test dataframe: PipelineModel_4b1ca3deb431353888f0\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Nothing has been added to this summarizer.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3330.evaluate.\n: java.lang.IllegalArgumentException: requirement failed: Nothing has been added to this summarizer.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.mllib.stat.MultivariateOnlineSummarizer.normL2(MultivariateOnlineSummarizer.scala:280)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr$lzycompute(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.SSerr(RegressionMetrics.scala:65)\n\tat org.apache.spark.mllib.evaluation.RegressionMetrics.r2(RegressionMetrics.scala:125)\n\tat org.apache.spark.ml.evaluation.RegressionEvaluator.evaluate(RegressionEvaluator.scala:88)\n\tat sun.reflect.GeneratedMethodAccessor116.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2d440811a8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprediction\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtvsModel_exper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_text_label_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# transform testing dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best model on test dataframe:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtvsModel_exper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# return best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mrsquared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluate r squared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---Linear Regression with hash IDF predictors: R Squared for testing dataset is %s ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrsquared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Training-validation completed in %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Nothing has been added to this summarizer.'"
     ]
    }
   ],
   "source": [
    "#  User parameters input for regression and wraping of functions from reading data to running the experiments\n",
    "\n",
    "#  the parameters for the experiments can be set below on the table\n",
    "parameters = {'Number_of_Features_hTF': [100000, 200000],\n",
    "              'Regression_Parameters': [0.1, 0.3],\n",
    "              'Regression_Iterations': [10, 20],\n",
    "              'Regression_ElasticParam': [0.1, 0.3],\n",
    "              'Train_Ratio':[0.6, 0.7, 0.8, 0.9],\n",
    "              'Feature_Experiment':['idf','hTF']}\n",
    "\n",
    "#  data transformations function call for both parliament reports and currency\n",
    "id_text_label=connect_xr_df(make_dataFrame(os.getcwd()+'/textfiles'),currency_df(data_xr))\n",
    "[id_text_label_train, id_text_label_test] = id_text_label.randomSplit([0.8, 0.2], 25) # split id_text_label into training (80%) and testing (20%) subsets, seed = 25\n",
    "\n",
    "#  Experiment loop changing the pipeline \n",
    "for ratio in parameters['Train_Ratio']: #  loop the ratio train split %\n",
    "    for feature in parameters['Feature_Experiment']: #  loop through the hashed idf dataframe and hashed dataframe\n",
    "        \n",
    "        pipeline=pipeline_fc(feature) #  call pipeline function \n",
    "        \n",
    "        print ('\\n***** Experiment initiated *****')\n",
    "        print ('\\n Parameters: Train Ratio('+str(ratio)+'),Feature :('+str(feature)+')')\n",
    "        \n",
    "        start_time = time.time() # initiate timing\n",
    "        \n",
    "        lr=linear_regression(feature) # call linear regression function according to feature testing\n",
    "        # set parameter grid\n",
    "        paramGrid_exper = ParamGridBuilder() \\\n",
    "            .addGrid(hashingTF.numFeatures, parameters['Number_of_Features_hTF']) \\\n",
    "            .addGrid(lr.regParam, parameters['Regression_Parameters']) \\\n",
    "            .addGrid(lr.maxIter, parameters['Regression_Iterations']) \\\n",
    "            .addGrid (lr.elasticNetParam,parameters['Regression_ElasticParam'])\\\n",
    "            .build()\n",
    "            \n",
    "        # train validation split\n",
    "        tvs_exper = TrainValidationSplit(estimator=pipeline,estimatorParamMaps=paramGrid_exper,evaluator =evaluator,trainRatio=ratio)\n",
    "        tvsModel_exper = tvs_exper.fit(id_text_label_train) # fit model to dataframe\n",
    "        \n",
    "        # R squared for prediction on training dataframe \n",
    "        prediction  = tvsModel_exper.transform(id_text_label_train) # transform training dataframe\n",
    "        print('Best model on training dataframe:',tvsModel_exper.bestModel) # return best model\n",
    "        rsquared = evaluator.evaluate(prediction) # evaluate r squared\n",
    "        print(\"---Linear Regression with hash IDF predictors: R Squared for training dataset is %s ---\" % (rsquared))\n",
    "        \n",
    "        # R squared for prediction on test dataframe \n",
    "        prediction  = tvsModel_exper.transform(id_text_label_test) # transform testing dataframe\n",
    "        print('Best model on test dataframe:',tvsModel_exper.bestModel) # return best model\n",
    "        rsquared = evaluator.evaluate(prediction) # evaluate r squared\n",
    "        print(\"---Linear Regression with hash IDF predictors: R Squared for testing dataset is %s ---\" % (rsquared))\n",
    "        \n",
    "        # R squared for prediction on total dataframe \n",
    "        print('Best model on total dataframe',tvsModel_exper.bestModel)\n",
    "        rsquared = evaluator.evaluate(tvsModel_exper.transform(id_text_label))\n",
    "        print(\"---Linear Regression with on total dataframe: R Squared is %s ---\" % (rsquared))\n",
    "        print(\"--- Training-validation completed in %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
