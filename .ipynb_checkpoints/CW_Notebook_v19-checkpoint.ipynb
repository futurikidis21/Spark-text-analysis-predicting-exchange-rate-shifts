{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432: Big Data - Coursework (Part II)\n",
    "\n",
    "## Predicting shifts in GBP-EUR exchange rates based on the content of UK parliamentary debates: A pySpark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alexandros Dimitrios Nalmpantis; Georgios Kyriakopoulos (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The pySpark code presented in this notebook aims to construct a modelling process, which predicts shifts in GBP-EUR exchange rates on the basis of political narrative. More specifically, the code implements a process that, schematically speaking, involves the following steps:\n",
    "\n",
    "(a) It scrapes the (almost daily) reports that record debates at the House of Commons and the House of Lords, as published on the UK Government website. The reports (available in PDF format) are converted into txt format, before removing common and usually uninformative words (i.e. stopwords), numeric figures, and punctuation symbols. File names and the remainder text content are converted into a data-frame. Following the construction of this dataframe, the content of the debate reports is tonekinsed and hashed, while term frequencies (TFs) and inverse document frequencies (IDFs) are calculated for the hashes.\n",
    "\n",
    "(b) It scrapes the (almost daily) EUR-GBP exchange rates, published on the Bank of England website. The content is written in a text file, which is then processed to remove blank space and to calculate the difference between each GBP-EUR exchange-rate value and its immediately previous exchange-rate value (i.e. *the exchange-rate shift*). Dates and exhange-rate shifts are then converted into a second data-frame.\n",
    "\n",
    "(c) It links the data frames constracted at (a) and (b) together, so that the content of the debate reports at a certain date is appended to the shift in GBP-EUR exchange rates recorded the day after. This new dataframe serves as the analytical input on the basis of which the main analysis is conducted.\n",
    "\n",
    "(d) Finally, it constructs  a machine learning pipeline, whereby the linear regression algorithm is trained (and validated) to predict exchange-rate shifts based on either the TF or the IDF of the (tokenised and hashed) content of the debate reports. Alternative parameterisation options are explored using a grid, to help optimise prediction.\n",
    "\n",
    "The findings of the analysis are highlighted within the anotation of this code and, broadly speaking, they appear to suggest that data-driven approaches (such as the one presented here) have the potential to detect statistical links  between  macroeconomic finacial-market parameters and topical political narratives. It is acknowledged that time-series modelling techniques (rather than linear regression) would be a more appropriate analytical approach, given the nature of the analysis dataset. However, for the purposes of this preliminary study we are (partially) adressing the issue of autocorrelations between data points by focusing the analysis on exchange-rate shifts rather than exchange rate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Modules needed for the analysis are imported below. Some modules may need to be installed with the following commands to a termninal: **pip install <\"name of module\">** eg: pip install tqdm  or **with conda install <\"name of module\">** eg: conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import modules for scraping links\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date,timedelta\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Import modules for downloading links\n",
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "# Import modules for parsing pdf's,progress bars and handling errors\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Import modules for spark ML and SQL\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "# import various operators\n",
    "from math import log\n",
    "import time\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from  stop_words import get_stop_words\n",
    "\n",
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set sparkcontext as sc\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Data collection and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###     2.1 Defining and calling wrapped procedures from scraping, downloading and converting parliamentary debates to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define if report data were provided ( yes - data were provided, no - data need to be scraped)\n",
    "trg='yes'\n",
    "\n",
    "    # set from which date to current date that the function will download reports in YYYY-M-D format below\n",
    "start_date = date(2016, 6, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Data were given====\n",
      "====Data were given====\n",
      "==Downloading and conversion to text files completed==\n"
     ]
    }
   ],
   "source": [
    "# Data control function that controls wether the data will be scraped or were provided by students\n",
    "def data_control(page,start_date,trg):\n",
    "    if trg=='yes':\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        html_page = urllib.request.urlopen(page) #request page with urllib packages\n",
    "        soup = BeautifulSoup(html_page) #pass the page to beautiful soup in order to extract the links contained in webpage \n",
    "        #print (soup) #visually inspect the html structure\n",
    "        hl = [] #set hyperlink array to store the extracted links\n",
    "        ##search html for hyperlinks starting with qna\n",
    "        for hyperlink in soup.findAll('a', attrs={'href': re.compile(\"^http://qna\")}): \n",
    "            hl.append(hyperlink.get('href')) #store the hyperlinks found on an array\n",
    "        #    print (link.get('href'))\n",
    "        \n",
    "        url=[hl[1][:-22]+'Lords-',hl[1][:-22]+'Commons-'] #take first result and cut the dates and category of either lords or commons\n",
    "    \n",
    "        #date interval search set and downloading of the pdf files\n",
    "        ##create interval search date\n",
    "        today=datetime.datetime.today() #today's date set\n",
    "        cur_date = date(today.year,today.month,today.day)  # set current date in format of YYYY-MM-DD\n",
    "    \n",
    "        dt = cur_date - start_date #calculate interval in days to use for loop\n",
    "        #make directory to downloaded files\n",
    "        try:\n",
    "            os.makedirs(os.getcwd()+'/parliament_practicals') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        #loop throught the interval with 1 day step and append the date to url along with categories of either house of Lords or Commons\n",
    "        for ul in url:\n",
    "        #    print ('Downloading: ',str('House of '+ul[112:-2]+'s'))\n",
    "            for i in tnrange(dt.days + 1,desc='Downloading: '+str(ul[112:-2]+'s')):\n",
    "                try: #test for errors and pass since there are dates that the House of Lords do not convene and HTTP request returns error; Also store results on folder parliament practicals\n",
    "                    filename = wget.download(ul+str(start_date + timedelta(days=i))+'.pdf',os.getcwd()+'/parliament_practicals')\n",
    "                except:\n",
    "                    next  \n",
    "    \n",
    "# function to convert the downloaded pdfs to text files\n",
    "def convert_pdf_to_text(trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory textfiles already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/textfiles') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        list_of_files=os.listdir(os.getcwd()+'/parliament_practicals') # create a list of pdf files to be converted\n",
    "        for i in tnrange(len(list_of_files),desc='Converting pdf to txt'): # iterate throught the files on the list and install progress bar\n",
    "            if list_of_files[i].endswith(\".pdf\"): # check that file input is pdf file\n",
    "                parsedPDF=parser.from_file(os.getcwd()+'/parliament_practicals/'+list_of_files[i]) # parse pdf file\n",
    "                text_file = open(os.getcwd()+'/textfiles/'+list_of_files[i][:-4]+'.txt', 'a') # create new filename with extension .txt\n",
    "                text_file.write(parsedPDF[\"content\"]) # write parsed pdf to text\n",
    "                text_file.close() # close text file\n",
    "            else: # if file other than pdf continue loop\n",
    "                next\n",
    "def download_practicals_convert(start_date,trg):\n",
    "    page=\"http://www.parliament.uk/business/publications/written-questions-answers-statements/daily-reports/\" # set link to parliament daily questions and answers reports\n",
    "    data_control(page,start_date,trg) # call set data function\n",
    "    convert_pdf_to_text(trg)# convert to pdf function\n",
    "    print ('==Downloading and conversion to text files completed==')\n",
    "    \n",
    "    # Call function to either download the data or set current folder as working folder...please make sure that\n",
    "    # if data are give then those should be stored on the folder: 'parliament_practicals'\n",
    "    # We suggest to run the scraping function since it only takes 2minutes for downloading a year of reports   \n",
    "download_practicals_convert(start_date,trg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Defining and calling wrapped procedures for scraping, downloading and converting time-series exchange-rate shifts to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define if exchange rate data were provided ( yes - data were provided, no - data need to be scraped)\n",
    "trg='no' # change this value to yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date    Rate\n",
      "0  1999-01-04 -0.0019\n",
      "1  1999-01-05  0.0082\n",
      "2  1999-01-06  0.0007\n",
      "3  1999-01-07  0.0068\n",
      "4  1999-01-08  0.0011\n"
     ]
    }
   ],
   "source": [
    "# define function to download exchange rates to text file in folder xr\n",
    "# Clean exchange data download function and transform to pandas dataframe\n",
    "def clean_ex(file_path):\n",
    "    data = pd.read_csv(file_path,sep=\" t \",header=None, encoding=\"ISO-8859-1\") # load the text file\n",
    "    remove_words=['Bank of England Statistical Interactive Database','Series 1 to 1','Spot exchange rate, Euro into Sterling','XUDLERS','Ã‚','Â','var w=window;if(w.performance||w.mozPerformance||w.msPerformance||w.webkitPerformance){var d=document;AKSB=w.AKSB||{},AKSB.q=AKSB.q||[],AKSB.mark=AKSB.mark||function(e,_){AKSB.q.push([\"mark\",e,_||(new Date).getTime()])},AKSB.measure=AKSB.measure||function(e,_,t){AKSB.q.push([\"measure\",e,_,t||(new Date).getTime()])},AKSB.done=AKSB.done||function(e){AKSB.q.push([\"done\",e])},AKSB.mark(\"firstbyte\",(new Date).getTime()),AKSB.prof={custid:\"445139\",ustr:\"\",originlat:\"0\",clientrtt:\"9\",ghostip:\"62.24.201.160\",ipv6:false,pct:\"10\",clientip:\"85.211.227.93\",requestid:\"4bf4eea\",region:\"16972\",protocol:\"\",blver:13,akM:\"r\",akN:\"\",akTT:\"O\",akTX:\"1\",akTI:\"4bf4eea\",ai:\"286733\",ra:\"false\",pmgn:\"\",pmgi:\"\",pmp:\"\",qc:\"\"},function(e){var _=d.createElement(\"script\");_.async=\"async\",_.src=e;var t=d.getElementsByTagName(\"script\"),t=t[t.length-1];t.parentNode.insertBefore(_,t)}((\"https:\"===d.location.protocol?\"https:\":\"http:\")+\"//ds-aksb-a.akamaihd.net/aksb.min.js\")}']\n",
    "    for word in remove_words: # remove words\n",
    "        data=data.replace(word,np.nan) # remove words\n",
    "    data=data.dropna()# drop nan\n",
    "    data.reset_index() # reset indices\n",
    "    rate=data.iloc[::2] # extract odd rows\n",
    "    date=data.iloc[1::2] # extract even rows\n",
    "    date.reset_index(inplace=True,drop=True) # reset indices\n",
    "    rate.reset_index(inplace=True,drop=True) # reset indices\n",
    "    x=pd.concat([date,rate],axis=1) # concatenate date and rates\n",
    "    x.columns=['Rate','Date'] # rename columns\n",
    "    x=x.dropna()# drop nan\n",
    "    x=x.drop_duplicates('Date') # drop duplicates\n",
    "    x['Date'] = pd.to_datetime(x['Date']) # convert date column to date\n",
    "    x[['Rate']] = x[['Rate']].apply(pd.to_numeric) #convert exchange rate to float\n",
    "    x = x.set_index('Date').diff() # calculate [rate(t+1) - rate(t)]\n",
    "    x.reset_index(inplace=True)# reset Date column\n",
    "    x['Date']=x['Date'].dt.strftime('%Y-%m-%d')# convert to string for join matching operations\n",
    "    x.Rate = x.Rate.shift(-1)# shift rate column by one day to account for the delay of the parliament report\n",
    "    x=x.dropna()# drop na\n",
    "    x.to_csv(os.getcwd()+'/xr/exchangeRates_diff.csv')# save to csv file\n",
    "    return x # return dataframe\n",
    "\n",
    "def download_xr(trg):\n",
    "    html= urllib.request.urlopen(\"http://www.bankofengland.co.uk/boeapps/iadb/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1963&TD=11&TM=Apr&TY=2017&VFD=Y&CSVF=TT&C=C8J&Filter=N&html.x=11&html.y=9\")\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "    else:\n",
    "        try: # test if directory xr already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/xr') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        soup_xr = BeautifulSoup(html)\n",
    "        xr = soup_xr.get_text()\n",
    "        #print(xr)\n",
    "        text_xr = open(os.getcwd()+'/xr/'+'exchangeRates'+'.txt', \"a\")\n",
    "        text_xr.write(xr)\n",
    "        text_xr.close()\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "        next\n",
    "    return data\n",
    "\n",
    "data_xr = download_xr(trg)\n",
    "print(data_xr.head()) # print head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task A: Select a dataset and make initial load and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of filename - text with numbers and punctuation removed\n",
    "\n",
    "# Set stop word parameters-for stopwords removal the stop_words pachage was used\n",
    "# The  StopWordsRemover from pyspark.ml.features was also tested extensively but was not effective or buggy\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "def remove_n_p(text): # function that removes punctuation and numbers as well lowercasing the text\n",
    "    text = re.sub(r'\\d+','', text) # remove numbers from texts with regular expressions <<<<<\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\W+', ' ', text)# remove punctuation from texts with regular expressions <<<<<\n",
    "    text=text.lower() # lowercase the text\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # remove stopwords\n",
    "    return text\n",
    "\n",
    "# extract date from filename function \n",
    "def trim_filename(filename):\n",
    "    date=filename[-14:-4] # extract the timestamp from end of the file\n",
    "    return date # return date\n",
    "    \n",
    "# sparksession added for spark dataframes\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def make_dataFrame(dirPath): # make a dataFrame with filename and text \n",
    "    ft_RDD = sc.wholeTextFiles(dirPath) # add code to create an RDD with wholeTextFiles\n",
    "    spm_t_RDD = ft_RDD.map(lambda ft: (trim_filename(ft[0]), remove_n_p(ft[1]))) # create RDD with filename and call remove_n_p function to text\n",
    "    file_text_df = spark.createDataFrame(spm_t_RDD,schema=['id','text']) # create a dataFrame - filename - text\n",
    "    return file_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|        id|                text|  label|\n",
      "+----------+--------------------+-------+\n",
      "|2017-02-24|daily report frid...|-0.0069|\n",
      "|2017-02-24|friday february p...|-0.0069|\n",
      "|2016-07-06|daily report wedn...| 0.0041|\n",
      "|2016-07-06|wednesday july p ...| 0.0041|\n",
      "|2017-03-08|daily report wedn...|-0.0044|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert currency pandas dataframe to Spark Dataframe and specify datatypes\n",
    "\n",
    "def currency_df(data_xr):# function to create dataframe\n",
    "    data_xr_DF=spark.createDataFrame(data_xr,schema=['Date','Rate']) # create dataframe\n",
    "    data_xr_DF=data_xr_DF.withColumn('Rate', data_xr_DF['Rate'].cast('float'))# convert rate to float\n",
    "    return data_xr_DF # return spark dataframe\n",
    "\n",
    "# function to join the two dataframes on dates, the exchange rate dates have been shifted back by one day\n",
    "# in order to account for the delay of the parliament publication\n",
    "def connect_xr_df(file_text_df,data_xr_DF): # the function takes the two inpurts of file_text_df and the data_xr_DF from the previous function\n",
    "    file_text_Date_rate=file_text_df.join(data_xr_DF,file_text_df.id==data_xr_DF.Date,'leftouter') # the exchange rates were connected to the timestamp of the parliament files with a matching left outer join\n",
    "    file_text_Date_rate.createOrReplaceTempView(\"temp\") # create a temporary sql view\n",
    "    file_text_Date_rate_sql = spark.sql(\"SELECT id,text,Rate as label FROM temp\") # select statement of the three columns required for analysis and relabeling\n",
    "    file_text_Date_rate_sql.show(5)\n",
    "    return file_text_Date_rate_sql # return the dataframe for analysis\n",
    "\n",
    "id_text_label=connect_xr_df(make_dataFrame(os.getcwd()+'/textfiles'),currency_df(data_xr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task B: Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id_text_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8fb20487e0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Step1: use tokenizer to split word into array and sql to select the filename - word_array created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_text_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Step2: make hashTF sparse vector with maximum 500 features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id_text_label' is not defined"
     ]
    }
   ],
   "source": [
    "#Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizer.transform(id_text_label)\n",
    "\n",
    "#Step2: make hashTF sparse vector with maximum 500 features\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "#Step 3: feed hash vector to calculate idf\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"idf\") \n",
    "\n",
    "#Step4: linear regression parameters\n",
    "lr_tf = LinearRegression()\\\n",
    "     .setFeaturesCol(\"features\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "lr_idf = LinearRegression()\\\n",
    "     .setFeaturesCol(\"idf\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "#Step 5: configure alternative pipelines \n",
    "pipeline_tf = Pipeline(stages=[tokenizer, hashingTF, lr_tf]) #with hash vector tf\n",
    "pipeline_idf = Pipeline(stages=[tokenizer, hashingTF, idf, lr_idf]) #with hash vector idf \n",
    "\n",
    "#Step 6: set exemplar parameter grid\n",
    "paramGrid_tf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [500]) \\\n",
    "    .addGrid(lr_tf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_tf.maxIter, [50]) \\\n",
    "    .addGrid (lr_tf.elasticNetParam,[0.8])\\\n",
    "    .build()\n",
    "\n",
    "paramGrid_idf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [500]) \\\n",
    "    .addGrid(lr_idf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_idf.maxIter, [50]) \\\n",
    "    .addGrid (lr_idf.elasticNetParam,[0.8])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task C: Evaluate the performance of your pipeline using training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "\n",
    "tvs_tf = TrainValidationSplit(estimator=pipeline_tf, \n",
    "                           estimatorParamMaps=paramGrid_tf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "tvsModel_tf = tvs_tf.fit(id_text_label)\n",
    "rsquared_tf = evaluator.evaluate(tvsModel_tf.transform(id_text_label))\n",
    "print(\"---Linear Regression with hash TF: R Squared is %s ---\" % (rsquared_tf))\n",
    "\n",
    "\n",
    "\n",
    "tvs_idf = TrainValidationSplit(estimator=pipeline_idf, \n",
    "                           estimatorParamMaps=paramGrid_idf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "tvsModel_idf = tvs_idf.fit(id_text_label)\n",
    "rsquared_idf = evaluator.evaluate(tvsModel_idf.transform(id_text_label))\n",
    "print(\"---Linear Regression with hash IDF: R Squared is %s ---\" % (rsquared_idf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task D: Implement a parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parameters = {'Number_of_Features_hTF': [50, 100],\n",
    "              'Regression_Parameters': [0.1, 0.3],\n",
    "              'Regression_Iterations': [10, 20],\n",
    "              'Regression_ElasticParam': [0.1, 0.3],\n",
    "              'Train_Ratio':[0.6, 0.7, 0.8, 0.9],\n",
    "              'Feature_Experiment':['idf','hTF']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build parameters required for experiments\n",
    "\n",
    "# Pipeline function to fit the process of building a pipeline on various inputs\n",
    "    \n",
    "def pipeline_fc(feature): # next step function for bulding pipeline\n",
    "    # Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "    tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "    \n",
    "    #Step2: make hashTF sparse vector\n",
    "    hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features') # set parameters for hashing\n",
    "    \n",
    "    #Step 3: feed hash vector to calculate idf\n",
    "    idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol='idf') # set parameters for idf\n",
    "    \n",
    "    #Step4: linear regression parameters and if statement for idf\n",
    "    \n",
    "    if feature=='hTF': # input for hashing only pipeline\n",
    "        lr = LinearRegression().setFeaturesCol('features').setLabelCol('label') #\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF, lr]) \n",
    "    else: #input for hashing-idf pipeline\n",
    "        lr = LinearRegression().setFeaturesCol('idf').setLabelCol('label')\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF,idf,lr]) \n",
    "    return pipeline # return pipeline\n",
    "    \n",
    "\n",
    "#  Step 5: parameter grid set from user input on parameter dictionary\n",
    "paramGrid_exper = ParamGridBuilder() \\\n",
    ".addGrid(hashingTF.numFeatures, parameters['Number_of_Features_hTF']) \\\n",
    "        .addGrid(lr_idf.regParam, parameters['Regression_Parameters']) \\\n",
    "        .addGrid(lr_idf.maxIter, parameters['Regression_Iterations']) \\\n",
    "        .addGrid (lr_idf.elasticNetParam,parameters['Regression_ElasticParam'])\\\n",
    "        .build()\n",
    "        \n",
    "\n",
    "#  Step 6: parameter grid set from user input on parameter dictionary\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # evaluator setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#  User parameters input for regression and wraping of functions from reading data to running the experiments\n",
    "\n",
    "#  the parameters for the experiments can be set below on the table\n",
    "parameters = {'Number_of_Features_hTF': [50, 100],\n",
    "              'Regression_Parameters': [0.1, 0.3],\n",
    "              'Regression_Iterations': [10, 20],\n",
    "              'Regression_ElasticParam': [0.1, 0.3],\n",
    "              'Train_Ratio':[0.6, 0.7, 0.8, 0.9],\n",
    "              'Feature_Experiment':['idf','hTF']}\n",
    "\n",
    "#  data transformations function call for both parliament reports and currency\n",
    "id_text_label=connect_xr_df(make_dataFrame(os.getcwd()+'/textfiles'),currency_df(data_xr))\n",
    "\n",
    "#  Experiment loop changing the pipeline \n",
    "\n",
    "for ratio in parameters['Train_Ratio']:\n",
    "    for feature in parameters['Feature_Experiment']:\n",
    "        pipeline=pipeline_fc(feature)\n",
    "        print ('\\n***** Experimen initiated *****')\n",
    "        print ('\\n Parameters: Train Ratio('+str(ratio)+'),Feature :('+str(feature)+')')\n",
    "        start_time = time.time()\n",
    "        tvs_exper = TrainValidationSplit(estimator=pipeline,estimatorParamMaps=paramGrid_exper,evaluator =evaluator,trainRatio=ratio)\n",
    "        tvsModel_exper = tvs_exper.fit(id_text_label)\n",
    "        print(\"--- Training-validation completed in %s seconds ---\" % (time.time() - start_time))\n",
    "        print(tvsModel_exper.bestModel)\n",
    "        rsquared_idf = evaluator.evaluate(tvsModel_exper.transform(id_text_label))\n",
    "        print(\"---Linear Regression with hash IDF: R Squared is %s ---\" % (evaluator.evaluate(tvsModel_exper.transform(id_text_label))))\n",
    "        #print(list(zip(tvsModel_exper.getEstimatorParamMaps())))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
