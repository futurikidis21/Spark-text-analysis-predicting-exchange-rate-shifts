{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432: Big Data - Coursework (Part II)\n",
    "\n",
    "## Predicting shifts in GBP-EUR exchange rates based on the content of UK parliamentary debates: A pySpark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alexandros Dimitrios Nalmpantis; Georgios Kyriakopoulos (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study objective and approach \n",
    "\n",
    "The pySpark code presented in this notebook aims to construct a modelling process, which tests if shifts in GBP-EUR exchange rates can be predicted on the basis of political speech. More specifically, the code implements a process that involves the following steps:\n",
    "\n",
    "(a) It scrapes the (almost daily) reports that record debates at the House of Commons and the House of Lords, as published on the UK Government website. The reports (available in PDF format) are converted into txt files and common (usually uninformative) words, numeric figures, and punctuation symbols are removed. File names and the remainder text content are converted into a data-frame and the content is tokenised and hashed, before computing term frequencies (TFs) and inverse document frequencies (IDFs)for the hashes.\n",
    "\n",
    "(b) It scrapes the (almost daily) EUR-GBP exchange rates, published on the Bank of England website. The content is written in a text file, which is then processed to remove blank space and to calculate the difference between each GBP-EUR exchange-rate value and its immediately previous exchange-rate value (i.e. *the exchange-rate shift*). Dates and exchange-rate shifts are then converted into a second data-frame.\n",
    "\n",
    "(c) It links the data frames constructed at (a) and (b) together, so that the content of the debate reports at a certain date is appended to the shift in GBP-EUR exchange rates recorded the day after. This new data-frame serves as the analytical input on the basis of which the main analysis is conducted.\n",
    "\n",
    "(d) Finally, it constructs a machine learning pipeline, whereby the linear regression models are trained, validated, and tested to predict exchange-rate shifts based on either the TF or the IDF of the (tokenised and hashed) content of the debate reports. Alternative parameterisation options are explored using a grid.\n",
    "\n",
    "The processes of data collection embedded in (a) and (b) depend on the format at which relevant data are published on the UK Government and the Bank of England websites. The functionality of the code that collects the data in this notebook was last confirmed on 23/04/2017 and can be affected by future changes in the way that information is published. Along with this notebook, we provide the datafiles scraped on 23/04/2017, so that the analysis can be repeated when this code is used / assessed by others.  \n",
    "\n",
    "The findings of the analysis are highlighted throughout the annotation in this notebook. Overall (and perhaps unsurprisingly), the findings suggest that political speech at the House of Lords and the House of commons is unsuccessful in predicting complex and dynamic macroeconomic financial-market parameters, such as the GBP-EUR exchange-rate shifts. \n",
    "\n",
    "Given the time-series nature of the data that this analysis considers, we have (partially) addressed the issue of autocorrelations when using linear regression analysis by focusing the analysis on the prediction of exchange-rate shifts (rather than the prediction of exchange rate values). We suggest that the analysis is revisited in the future using perhaps more appropriate analytical techniques, such as Autoregressive Integrated Moving Average time-series models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1 || Analysis modules\n",
    "\n",
    "The modules needed for the analysis are imported below. Some modules may need to be installed to the terminal with the following commands: **pip install <\"name of module\">** eg: pip install tqdm  or **with conda install <\"name of module\">** eg: conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Modules used for scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date,timedelta\n",
    "import os\n",
    "\n",
    "# Modules used for downloads\n",
    "import wget\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modules used for for parsing PDFs\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Spark ML and SQL modules\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF, IDF\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Other modules\n",
    "from math import log\n",
    "import time\n",
    "from pprint import pprint\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from  stop_words import get_stop_words\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set sparkcontext as sc\n",
    "#sc=SparkContext() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2 || Data collection and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###     2.1 Procedures for scraping, downloading, and converting parliamentary-debate reports into text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define if readily available debate-report data will be used (trg= 'yes') or if they will be scraped (trg ='no')\n",
    "trg='no' #change this value to 'yes' or 'no'\n",
    "\n",
    "# Set date from which onwards debate-reports with be downloaded in YYYY-M-D format\n",
    "start_date = date(2016, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data_control function that controls whether the readily available debate-report data will be used or if they will be scraped\n",
    "def data_control(page,start_date,trg):\n",
    "    if trg=='yes':\n",
    "        os.chdir(os.getcwd)\n",
    "    else:\n",
    "        html_page = urllib.request.urlopen(page) #request page with urllib packages\n",
    "        soup = BeautifulSoup(html_page) #pass the page to beautiful soup in order to extract the links contained in webpage \n",
    "        #print (soup) #visually inspect the html structure\n",
    "        hl = [] #set hyperlink array to store the extracted links\n",
    "        ##search html for hyperlinks starting with qna\n",
    "        for hyperlink in soup.findAll('a', attrs={'href': re.compile(\"^http://qna\")}): \n",
    "            hl.append(hyperlink.get('href')) #store the hyperlinks found on an array\n",
    "        #    print (link.get('href'))\n",
    "        \n",
    "        url=[hl[1][:-22]+'Lords-',hl[1][:-22]+'Commons-'] #take first result and cut the dates and category of either lords or commons\n",
    "    \n",
    "        ##create interval search date\n",
    "        today=datetime.datetime.today() #today's date set\n",
    "        cur_date = date(today.year,today.month,today.day)  # set current date in format of YYYY-MM-DD\n",
    "    \n",
    "        dt = cur_date - start_date #calculate interval in days to use for loop\n",
    "        ##make directory to downloaded files\n",
    "        try:\n",
    "            os.makedirs(os.getcwd()+'/parliament_practicals') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        ##loop throught the interval with 1 day step and append the date to url along with categories of either House of Lords or Commons\n",
    "        for ul in url:\n",
    "        #    print ('Downloading: ',str('House of '+ul[112:-2]+'s'))\n",
    "            for i in tnrange(dt.days + 1,desc='Downloading: '+str(ul[112:-2]+'s')):\n",
    "                try: #test for errors and pass since there are dates that the House of Lords do not convene and HTTP request returns error; Also store results on folder parliament practicals\n",
    "                    filename = wget.download(ul+str(start_date + timedelta(days=i))+'.pdf',os.getcwd()+'/parliament_practicals')\n",
    "                except:\n",
    "                    next  \n",
    "    \n",
    "# Function to convert the downloaded pdfs to text files\n",
    "def convert_pdf_to_text(trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory textfiles already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/textfiles') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        list_of_files=os.listdir(os.getcwd()+'/parliament_practicals') # create a list of pdf files to be converted\n",
    "        for i in tnrange(len(list_of_files),desc='Converting pdf to txt'): # iterate throught the files on the list and install progress bar\n",
    "            if list_of_files[i].endswith(\".pdf\"): # check that file input is pdf file\n",
    "                parsedPDF=parser.from_file(os.getcwd()+'/parliament_practicals/'+list_of_files[i]) # parse pdf file\n",
    "                text_file = open(os.getcwd()+'/textfiles/'+list_of_files[i][:-4]+'.txt', 'a') # create new filename with extension .txt\n",
    "                text_file.write(parsedPDF[\"content\"]) # write parsed pdf to text\n",
    "                text_file.close() # close text file\n",
    "            else: # if file other than pdf continue loop\n",
    "                next\n",
    "def download_practicals_convert(start_date,trg):\n",
    "    page=\"http://www.parliament.uk/business/publications/written-questions-answers-statements/daily-reports/\" # set link to parliament daily questions and answers reports\n",
    "    data_control(page,start_date,trg) # call set data function\n",
    "    convert_pdf_to_text(trg)# convert to pdf function\n",
    "    print ('==Downloading and conversion to text files completed==')\n",
    "    \n",
    "# Call function to either download the data or set current folder as working folder...please make sure that\n",
    "# if data are give then those should be stored on the folder: 'parliament_practicals'\n",
    "\n",
    "download_practicals_convert(start_date,trg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2. Procedures for scraping, downloading, and converting exchange rates to exchange-rate shifts and storing in a data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define if readily available exchange-rate data will be used (trg= 'yes') or if they will be scraped (trg ='no')\n",
    "trg='yes' # change this value to yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Data were given====\n",
      "         Date    Rate\n",
      "0  1999-01-04 -0.0019\n",
      "1  1999-01-05  0.0082\n",
      "2  1999-01-06  0.0007\n",
      "3  1999-01-07  0.0068\n",
      "4  1999-01-08  0.0011\n"
     ]
    }
   ],
   "source": [
    "# Clean exchange data download function and transform to pandas dataframe\n",
    "\n",
    "def clean_ex(file_path):\n",
    "    data = pd.read_csv(file_path,sep=\" t \",header=None, encoding=\"ISO-8859-1\") # load the text file\n",
    "    remove_words=['Bank of England Statistical Interactive Database','Series 1 to 1','Spot exchange rate, Euro into Sterling','XUDLERS','Ã‚','Â']\n",
    "    for word in remove_words: # remove words\n",
    "        data=data.replace(word,np.nan) # remove words\n",
    "    data=data.dropna()# drop nan\n",
    "    data.reset_index() # reset indices\n",
    "    rate=data.iloc[::2] # extract odd rows\n",
    "    date=data.iloc[1::2] # extract even rows\n",
    "    date.reset_index(inplace=True,drop=True) # reset indices\n",
    "    rate.reset_index(inplace=True,drop=True) # reset indices\n",
    "    x=pd.concat([date,rate],axis=1) # concatenate date and rates\n",
    "    x.columns=['Rate','Date'] # rename columns\n",
    "    x=x.dropna()# drop na\n",
    "    x['Date'] = pd.to_datetime(x['Date']) # convert date column to date\n",
    "    x[['Rate']] = x[['Rate']].apply(pd.to_numeric) #convert exchange rate to float\n",
    "    x.dtypes #check data types\n",
    "    x = x.set_index('Date').diff() # calculate [rate(t+1) - rate(t)]\n",
    "    x.reset_index(inplace=True)# reset Date column\n",
    "    x['Date']=x['Date'].dt.strftime('%Y-%m-%d')# convert to string for join matching operations\n",
    "    x.Rate = x.Rate.shift(-1)# shift rate column by one day to account for the delay of the parliament report\n",
    "    x=x.dropna()# drop na\n",
    "    x=x.drop_duplicates('Date') # drop duplicates\n",
    "    x.to_csv(os.getcwd()+'/xr/exchangeRates_diff.csv')# save to csv file\n",
    "    return x # return dataframe\n",
    "\n",
    "# Define function to download exchange rates to text file in folder xr\n",
    "def download_xr(trg):\n",
    "    html= urllib.request.urlopen(\"http://www.bankofengland.co.uk/boeapps/iadb/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1963&TD=11&TM=Apr&TY=2017&VFD=Y&CSVF=TT&C=C8J&Filter=N&html.x=11&html.y=9\")\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "    else:\n",
    "        try: # test if directory xr already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/xr') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        soup_xr = BeautifulSoup(html)\n",
    "        xr = soup_xr.get_text()\n",
    "        #print(xr)\n",
    "        text_xr = open(os.getcwd()+'/xr/'+'exchangeRates'+'.txt', \"a\")\n",
    "        text_xr.write(xr)\n",
    "        text_xr.close()\n",
    "        data=clean_ex(os.getcwd()+'/xr/exchangeRates.txt')\n",
    "        next\n",
    "    return data\n",
    "\n",
    "data_xr = download_xr(trg)\n",
    "print(data_xr.head()) # print head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3 || Defining machine-learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1 Collate analysis dataset, run initial load, and perform transformations [Task A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of filename - text with numbers and punctuation removed\n",
    "\n",
    "# Set stop word parameters-for stopwords removal the stop_words pachage was used\n",
    "# The  StopWordsRemover from pyspark.ml.features was also tested extensively but was buggy / not effective\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "def remove_n_p(text): # function that removes punctuation and numbers as well lowercasing the text\n",
    "    text = re.sub(r'\\d+','', text) # remove numbers from texts with regular expressions\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\W+', ' ', text)# remove punctuation from texts with regular expressions\n",
    "    text=text.lower() # lowercase the text\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # remove stopwords\n",
    "    return text\n",
    "\n",
    "# Extract date from filename function \n",
    "def trim_filename(filename):\n",
    "    date=filename[-14:-4] # extract the timestamp from end of the file\n",
    "    return date # return date\n",
    "    \n",
    "# SparkSession added for spark dataframes\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def make_dataFrame(dirPath): # make a dataFrame with filename and text \n",
    "    ft_RDD = sc.wholeTextFiles(dirPath) # add code to create an RDD with wholeTextFiles\n",
    "    spm_t_RDD = ft_RDD.map(lambda ft: (trim_filename(ft[0]), remove_n_p(ft[1]))) # create RDD with filename and call remove_n_p function to text\n",
    "    file_text_df = spark.createDataFrame(spm_t_RDD,schema=['id','text']) # create a dataFrame - filename - text\n",
    "    return file_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|        id|                text|  label|\n",
      "+----------+--------------------+-------+\n",
      "|2017-02-24|daily report frid...|-0.0069|\n",
      "|2017-02-24|friday february p...|-0.0069|\n",
      "|2016-07-06|daily report wedn...| 0.0041|\n",
      "|2016-07-06|wednesday july p ...| 0.0041|\n",
      "|2017-03-08|daily report wedn...|-0.0044|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert currency pandas dataframe to Spark dataframe and specify datatypes\n",
    "\n",
    "def currency_df(data_xr):# function to create dataframe\n",
    "    data_xr_DF=spark.createDataFrame(data_xr,schema=['Date','Rate']) # create dataframe\n",
    "    data_xr_DF=data_xr_DF.withColumn('Rate', data_xr_DF['Rate'].cast('float'))# convert rate to float\n",
    "    return data_xr_DF # return spark dataframe\n",
    "\n",
    "# function to join the two dataframes on dates. Exchange rate dates are shifted back by one day\n",
    "# in order to account for the delay of the parliament publication\n",
    "def connect_xr_df(file_text_df,data_xr_DF): # the function takes the two inpurts of file_text_df and the data_xr_DF from the previous function\n",
    "    file_text_Date_rate=file_text_df.join(data_xr_DF,file_text_df.id==data_xr_DF.Date,'left_outer') # the exchange rates were connected to the timestamp of the parliament files with a matching left outer join\n",
    "    file_text_Date_rate.createOrReplaceTempView(\"temp\") # create a temporary sql view\n",
    "    file_text_Date_rate_sql = spark.sql(\"SELECT id,text,Rate as label FROM temp\") # select statement of the three columns required for analysis and relabeling\n",
    "    file_text_Date_rate_sql.show(5)\n",
    "    return file_text_Date_rate_sql # return the dataframe for analysis\n",
    "\n",
    "id_text_label=connect_xr_df(make_dataFrame(os.getcwd()+'/textfiles'),currency_df(data_xr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2 Implement a machine-learning pipeline in Spark, including feature extractors, transformers, and selectors [Task B] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "#Step2: make hashTF \n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "#Step 3: feed hash vector to calculate idf\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"idf\") \n",
    "\n",
    "#Step4: linear regression parameters\n",
    "lr_tf = LinearRegression()\\\n",
    "         .setFeaturesCol(\"features\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "lr_idf = LinearRegression()\\\n",
    "     .setFeaturesCol(\"idf\")\\\n",
    "     .setLabelCol('label')\n",
    "\n",
    "#Step 5: configure alternative pipelines \n",
    "pipeline_tf = Pipeline(stages=[tokenizer, hashingTF, lr_tf]) #with hash vector tf\n",
    "pipeline_idf = Pipeline(stages=[tokenizer, hashingTF, idf, lr_idf]) #with hash vector idf \n",
    "\n",
    "#Step 6: set exemplar parameter grid\n",
    "paramGrid_tf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000]) \\\n",
    "    .addGrid(lr_tf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_tf.maxIter, [50]) \\\n",
    "    .addGrid (lr_tf.elasticNetParam,[0.8])\\\n",
    "    .build()\n",
    "\n",
    "paramGrid_idf = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000]) \\\n",
    "    .addGrid(lr_idf.regParam, [0.3]) \\\n",
    "    .addGrid(lr_idf.maxIter, [50]) \\\n",
    "    .addGrid (lr_idf.elasticNetParam,[0.8])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.3 Evaluate pipeline performance using training and test datasets [Task C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Train-Validation\n",
      "finished Train-Validation\n",
      "PipelineModel_401398cbedea7e2a888d\n",
      "---Linear Regression with hash TF predictors: R Squared for training dataset is 1.1102230246251565e-16 ---\n",
      "PipelineModel_401398cbedea7e2a888d\n",
      "---Linear Regression with hash TF predictors: R Squared for testing dataset is -0.05000512356683151 ---\n",
      "starting Train-Validation\n",
      "finished Train-Validation\n",
      "PipelineModel_463cab1342fb343af0bc\n",
      "---Linear Regression with hash IDF predictors: R Squared for training dataset is 1.1102230246251565e-16 ---\n",
      "PipelineModel_463cab1342fb343af0bc\n",
      "---Linear Regression with hash IDF predictors: R Squared for testing dataset is -0.05000512356683129 ---\n"
     ]
    }
   ],
   "source": [
    "[id_text_label_train, id_text_label_test] = id_text_label.randomSplit([0.8, 0.2], 25) # split id_text_label into training (80%) and testing (20%) subsets, seed = 25\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # Use R squared to evaluate performance of models (% of variance in xr shifts explained by predictors)\n",
    "\n",
    "### Modeling with pipeline_tf\n",
    "tvs_tf = TrainValidationSplit(estimator=pipeline_tf, \n",
    "                           estimatorParamMaps=paramGrid_tf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "# Run TrainValidationSplit on training dataset\n",
    "print('starting Train-Validation')\n",
    "tvsModel_tf = tvs_tf.fit(id_text_label_train)\n",
    "print('finished Train-Validation')\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_tf.transform(id_text_label_train)\n",
    "print(tvsModel_tf.bestModel)\n",
    "rsquared_tf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash TF predictors: R Squared for training dataset is %s ---\" % (rsquared_tf))\n",
    "\n",
    "# R squared for prediction on testing dataframe \n",
    "prediction  = tvsModel_tf.transform(id_text_label_test)\n",
    "print(tvsModel_tf.bestModel)\n",
    "rsquared_tf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash TF predictors: R Squared for testing dataset is %s ---\" % (rsquared_tf))\n",
    "\n",
    "### Modeling with pipeline pipeline_idf\n",
    "\n",
    "tvs_idf = TrainValidationSplit(estimator=pipeline_idf, \n",
    "                           estimatorParamMaps=paramGrid_idf,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "print('starting Train-Validation')\n",
    "tvsModel_idf = tvs_idf.fit(id_text_label_train)\n",
    "print('finished Train-Validation')\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_idf.transform(id_text_label_train)\n",
    "print(tvsModel_idf.bestModel)\n",
    "rsquared_idf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash IDF predictors: R Squared for training dataset is %s ---\" % (rsquared_idf))\n",
    "\n",
    "# R squared for prediction on training dataframe \n",
    "prediction  = tvsModel_idf.transform(id_text_label_test)\n",
    "print(tvsModel_idf.bestModel)\n",
    "rsquared_idf = evaluator.evaluate(prediction)\n",
    "print(\"---Linear Regression with hash IDF predictors: R Squared for testing dataset is %s ---\" % (rsquared_idf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.4 Experiment with and evaluate alternative pipelines using grid [Task D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build parameters required for experiments\n",
    "\n",
    "# Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "#Step2: make hashTF sparse vector\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features') # set parameters for hashing\n",
    "    \n",
    "#Step 3: feed hashTF vector to calculate idf\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol='idf') # set parameters for idf\n",
    "\n",
    "def linear_regression(feature):\n",
    "    #Step4: linear regression parameters and if statement for idf\n",
    "    if feature=='hTF': # input for hashing only pipeline\n",
    "        lr = LinearRegression().setFeaturesCol('features').setLabelCol('label')\n",
    "    else: #input for hashing-idf pipeline\n",
    "        lr = LinearRegression().setFeaturesCol('idf').setLabelCol('label')\n",
    "    return lr\n",
    "\n",
    "# Pipeline function to fit the process of building a pipeline on various inputs\n",
    "def pipeline_fc(feature): # next step function for bulding pipeline\n",
    "    #Step5: set pipeline according to feature\n",
    "    if feature=='hTF': # input for hashing only pipeline\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF, linear_regression(feature)]) \n",
    "    else: #input for hashing-idf pipeline\n",
    "        pipeline = Pipeline(stages=[tokenizer, hashingTF,idf,linear_regression(feature)])\n",
    "    return pipeline # return pipeline\n",
    "\n",
    "#  Step 6: parameter grid set from user input on parameter dictionary\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # evaluator setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n",
      "|        id|                text|  label|\n",
      "+----------+--------------------+-------+\n",
      "|2017-02-24|daily report frid...|-0.0069|\n",
      "|2017-02-24|friday february p...|-0.0069|\n",
      "|2016-07-06|daily report wedn...| 0.0041|\n",
      "|2016-07-06|wednesday july p ...| 0.0041|\n",
      "|2017-03-08|daily report wedn...|-0.0044|\n",
      "+----------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "***** Experiment initiated *****\n",
      "\n",
      " Parameters: Train Ratio(0.7),Feature :(hTF)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3086.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5314.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5314.0 (TID 278982, localhost, executor driver): org.apache.spark.SparkException: Python worker did not connect back in time\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:138)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:133)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2405)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2404)\r\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2404)\r\n\tat org.apache.spark.ml.regression.LinearRegressionSummary.numInstances$lzycompute(LinearRegression.scala:683)\r\n\tat org.apache.spark.ml.regression.LinearRegressionSummary.numInstances(LinearRegression.scala:683)\r\n\tat org.apache.spark.ml.regression.LinearRegressionSummary.<init>(LinearRegression.scala:687)\r\n\tat org.apache.spark.ml.regression.LinearRegressionTrainingSummary.<init>(LinearRegression.scala:575)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:396)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:76)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\r\n\tat sun.reflect.GeneratedMethodAccessor184.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker did not connect back in time\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:138)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:133)\r\n\t... 31 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0a2d477615dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[1;31m# train validation split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtvs_exper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainValidationSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mestimatorParamMaps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparamGrid_exper\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainRatio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtvsModel_exper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtvs_exper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_text_label_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fit model to dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[1;31m# R squared for prediction on training dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\ml\\tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumModels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3086.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5314.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5314.0 (TID 278982, localhost, executor driver): org.apache.spark.SparkException: Python worker did not connect back in time\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:138)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:133)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\r\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2405)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2404)\r\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2404)\r\n\tat org.apache.spark.ml.regression.LinearRegressionSummary.numInstances$lzycompute(LinearRegression.scala:683)\r\n\tat org.apache.spark.ml.regression.LinearRegressionSummary.numInstances(LinearRegression.scala:683)\r\n\tat org.apache.spark.ml.regression.LinearRegressionSummary.<init>(LinearRegression.scala:687)\r\n\tat org.apache.spark.ml.regression.LinearRegressionTrainingSummary.<init>(LinearRegression.scala:575)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:396)\r\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:76)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\r\n\tat sun.reflect.GeneratedMethodAccessor184.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker did not connect back in time\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:138)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:133)\r\n\t... 31 more\r\n"
     ]
    }
   ],
   "source": [
    "#  User parameters input for regression and wraping of functions from reading data to running the experiments\n",
    "\n",
    "#  the parameters for the experiments can be set below on the table\n",
    "parameters = {'Number_of_Features_hTF': [100000, 200000],\n",
    "              'Regression_Parameters': [0.1, 0.3],\n",
    "              'Regression_Iterations': [10, 20],\n",
    "              'Regression_ElasticParam': [0.1, 0.8],\n",
    "              'Train_Ratio':[0.7, 0.8],\n",
    "              'Feature_Experiment':['hTF', 'idf']}\n",
    "\n",
    "#  data transformations function call for both parliament reports and currency\n",
    "id_text_label=connect_xr_df(make_dataFrame(os.getcwd()+'/textfiles'),currency_df(data_xr))\n",
    "[id_text_label_train, id_text_label_test] = id_text_label.randomSplit([0.8, 0.2], 25) # split id_text_label into training (80%) and testing (20%) subsets, seed = 25\n",
    "\n",
    "#  Experiment loop changing the pipeline \n",
    "for ratio in parameters['Train_Ratio']: #  loop the ratio train split %\n",
    "    for feature in parameters['Feature_Experiment']: #  loop through the hashed idf dataframe and hashed dataframe\n",
    "        \n",
    "        pipeline=pipeline_fc(feature) #  call pipeline function \n",
    "        \n",
    "            print ('\\n***** Experiment initiated *****')\n",
    "        print ('\\n Parameters: Train Ratio('+str(ratio)+'),Feature :('+str(feature)+')')\n",
    "        \n",
    "        start_time = time.time() # initiate timing\n",
    "        \n",
    "        lr=linear_regression(feature) # call linear regression function according to feature testing\n",
    "        # set parameter grid\n",
    "        paramGrid_exper = ParamGridBuilder() \\\n",
    "            .addGrid(hashingTF.numFeatures, parameters['Number_of_Features_hTF']) \\\n",
    "            .addGrid(lr.regParam, parameters['Regression_Parameters']) \\\n",
    "            .addGrid(lr.maxIter, parameters['Regression_Iterations']) \\\n",
    "            .addGrid (lr.elasticNetParam,parameters['Regression_ElasticParam'])\\\n",
    "            .build()\n",
    "            \n",
    "        # train validation split\n",
    "        tvs_exper = TrainValidationSplit(estimator=pipeline,estimatorParamMaps=paramGrid_exper,evaluator =evaluator,trainRatio=ratio)\n",
    "        tvsModel_exper = tvs_exper.fit(id_text_label_train) # fit model to dataframe\n",
    "        \n",
    "        # R squared for prediction on training dataframe \n",
    "        prediction  = tvsModel_exper.transform(id_text_label_train) # transform training dataframe\n",
    "        print('Best model on training dataframe:',tvsModel_exper.bestModel) # return best model\n",
    "        rsquared = evaluator.evaluate(prediction) # evaluate r squared\n",
    "        print(\"---Linear Regression: R Squared for training dataset is %s ---\" % (rsquared))\n",
    "        \n",
    "        # R squared for prediction on test dataframe \n",
    "        prediction  = tvsModel_exper.transform(id_text_label_test) # transform testing dataframe\n",
    "        print('Best model on test dataframe:',tvsModel_exper.bestModel) # return best model\n",
    "        rsquared = evaluator.evaluate(prediction) # evaluate r squared\n",
    "        print(\"---Linear Regression: R Squared for testing dataset is %s ---\" % (rsquared))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Train-Validation with training to validation ratio = (0.7)\n",
      "finished Train-Validation\n",
      "PipelineModel_4aed8d38e362b986c773\n",
      "---Linear Regression with hash TF predictors: R Squared for training dataset is 0.0 ---\n",
      "PipelineModel_4aed8d38e362b986c773\n",
      "---Linear Regression with hash TF predictors: R Squared for testing dataset is -0.05000512356683151 ---\n",
      "starting Train-Validation with training to validation ratio = (0.8)\n"
     ]
    }
   ],
   "source": [
    "# TASK D SIMPLISTIC ALTERNATIVE ***************************************************************************\n",
    "\n",
    "#Set experiment parameter grid\n",
    "paramGrid_tf_exp = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000, 200000]) \\\n",
    "    .addGrid(lr_tf.regParam, [0.1, 0.3]) \\\n",
    "    .addGrid(lr_tf.maxIter, [20, 50]) \\\n",
    "    .addGrid (lr_tf.elasticNetParam,[0.1, 0.8])\\\n",
    "    .build()\n",
    "\n",
    "paramGrid_idf_exp = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100000, 200000]) \\\n",
    "    .addGrid(lr_idf.regParam, [0.1, 0.3]) \\\n",
    "    .addGrid(lr_idf.maxIter, [20, 50]) \\\n",
    "    .addGrid (lr_idf.elasticNetParam,[0.1, 0.8])\\\n",
    "    .build()\n",
    "\n",
    "[id_text_label_train, id_text_label_test] = id_text_label.randomSplit([0.8, 0.2], 25) # split id_text_label into training (80%) and testing (20%) subsets, seed = 25\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"r2\", labelCol=\"label\", predictionCol=\"prediction\") # Use R squared to evaluate performance of models (% of variance in xr shifts explained by predictors)\n",
    "\n",
    "### Experiments with pipeline_tf\n",
    "\n",
    "for ratio in [0.7, 0.8]:\n",
    "    tvs_tf_exp = TrainValidationSplit(estimator=pipeline_tf,\n",
    "                           estimatorParamMaps=paramGrid_tf_exp,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=ratio) # 80% of the data will be used for training, 20% for validation\n",
    "    # Run TrainValidationSplit on training dataset\n",
    "    print('starting Train-Validation with training to validation ratio = ('+str(ratio)+')')\n",
    "    tvsModel_tf_exp = tvs_tf_exp.fit(id_text_label_train)\n",
    "    print('finished Train-Validation')\n",
    "    # R squared for prediction on training dataframe \n",
    "    prediction  = tvsModel_tf_exp.transform(id_text_label_train)\n",
    "    print(tvsModel_tf_exp.bestModel)\n",
    "    rsquared_tf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash TF predictors: R Squared for training dataset is %s ---\" % (rsquared_tf_exp))\n",
    "    # R squared for prediction on testing dataframe \n",
    "    prediction  = tvsModel_tf_exp.transform(id_text_label_test)\n",
    "    print(tvsModel_tf_exp.bestModel)\n",
    "    rsquared_tf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash TF predictors: R Squared for testing dataset is %s ---\" % (rsquared_tf_exp))\n",
    "\n",
    "### Experiments with pipeline pipeline_idf\n",
    "\n",
    "for ratio in [0.7, 0.8]:\n",
    "    tvs_idf_exp = TrainValidationSplit(estimator=pipeline_idf,\n",
    "                           estimatorParamMaps=paramGrid_idf_exp,\n",
    "                           evaluator =evaluator,\n",
    "                           trainRatio=ratio) # 80% of the data will be used for training, 20% for validation\n",
    "    # Run TrainValidationSplit on training dataset\n",
    "    print('starting Train-Validation with training to validation ratio = ('+str(ratio)+')')\n",
    "    tvsModel_idf_exp = tvs_idf_exp.fit(id_text_label_train)\n",
    "    print('finished Train-Validation')\n",
    "    # R squared for prediction on training dataframe \n",
    "    prediction  = tvsModel_idf_exp.transform(id_text_label_train)\n",
    "    print(tvsModel_idf_exp.bestModel)\n",
    "    rsquared_idf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash IDF predictors: R Squared for training dataset is %s ---\" % (rsquared_idf_exp))\n",
    "    # R squared for prediction on testing dataframe \n",
    "    prediction  = tvsModel_idf_exp.transform(id_text_label_test)\n",
    "    print(tvsModel_idf_exp.bestModel)\n",
    "    rsquared_idf_exp = evaluator.evaluate(prediction)\n",
    "    print(\"---Linear Regression with hash IDF predictors: R Squared for testing dataset is %s ---\" % (rsquared_idf_exp))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
