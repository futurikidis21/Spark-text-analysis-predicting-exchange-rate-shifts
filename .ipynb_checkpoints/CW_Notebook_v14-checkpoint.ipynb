{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# INM432: Big Data - Coursework (Part II)\n",
    "\n",
    "## Predicting shifts in GBP-EUR exchange rates based on the content of UK parliamentary debates: A pySpark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alexandros Dimitrios Nalmpantis; Georgios Kyriakopoulos (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Abstract_** This notebook presents pySpark code that implements a machine-learning pipeline, which (a) scrapes and processes daily reports on debates at the House of Commons and the House of Lords; (b) scrapes daily GBP-EUR exchange rates and calculates their day-to-day shifts; and (c) links the two to train a regression-based algorithm that predicts the latter from the former. The analysis focuses on the prediction exchange-rate shifts(instead of raw exchange rates) to overcome the implications of autocorrelation for the modelling process. Alternative model hyperparameters are systematically assessed using a grid-search approach, which involves training, validating, and testing the performance of alternative model specifications.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Modules needed for the analysis are imported below. Some modules may need to be installed with the following commands to a termninal: **pip install <\"name of module\">** eg: pip install tqdm  or **with conda install <\"name of module\">** eg: conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import modules for scraping links\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date,timedelta\n",
    "import os\n",
    "\n",
    "# Import modules for downloading links\n",
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "# Import midules for parsing pdf's,progress bars and handling errors\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Import modules for spark ML, math and operators\n",
    "import re\n",
    "from operator import add\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF, IDF\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer,CountVectorizerModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from math import log\n",
    "import time\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "# Set sparkcontext as sc\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Data collection and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###     2.1 Defining and calling wrapped procedures from scraping, downloading and converting parliamentary debates to text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data control function that controls wether the data will be scraped or were provided by students\n",
    "def data_control(page,start_date,trg):\n",
    "    if trg=='yes':\n",
    "        os.chdir(os.getcwd)\n",
    "    else:\n",
    "        html_page = urllib.request.urlopen(page) #request page with urllib packages\n",
    "        soup = BeautifulSoup(html_page) #pass the page to beautiful soup in order to extract the links contained in webpage \n",
    "        #print (soup) #visually inspect the html structure\n",
    "        hl = [] #set hyperlink array to store the extracted links\n",
    "        ##search html for hyperlinks starting with qna\n",
    "        for hyperlink in soup.findAll('a', attrs={'href': re.compile(\"^http://qna\")}): \n",
    "            hl.append(hyperlink.get('href')) #store the hyperlinks found on an array\n",
    "        #    print (link.get('href'))\n",
    "        \n",
    "        url=[hl[1][:-20]+'Lords-',hl[1][:-20]+'Commons-'] #take first result and cut the dates and category of either lords or commons\n",
    "    \n",
    "        #date interval search set and downloading of the pdf files\n",
    "        ##create interval search date\n",
    "        today=datetime.datetime.today() #today's date set\n",
    "        cur_date = date(today.year,today.month,today.day)  # set current date in format of YYYY-MM-DD\n",
    "    \n",
    "        dt = cur_date - start_date #calculate interval in days to use for loop\n",
    "        #make directory to downloaded files\n",
    "        try:\n",
    "            os.makedirs(os.getcwd()+'/parliament_practicals') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        #loop throught the interval with 1 day step and append the date to url along with categories of either house of Lords or Commons\n",
    "        for ul in url:\n",
    "        #    print ('Downloading: ',str('House of '+ul[112:-2]+'s'))\n",
    "            for i in tnrange(dt.days + 1,desc='Downloading: '+str(ul[112:-2]+'s')):\n",
    "                try: #test for errors and pass since there are dates that the House of Lords do not convene and HTTP request returns error; Also store results on folder parliament practicals\n",
    "                    filename = wget.download(ul+str(start_date + timedelta(days=i))+'.pdf',os.getcwd()+'/parliament_practicals')\n",
    "                except:\n",
    "                    next  \n",
    "    \n",
    "# function to convert the downloaded pdfs to text files\n",
    "def convert_pdf_to_text(trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory textfiles already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/textfiles') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        list_of_files=os.listdir(os.getcwd()+'/parliament_practicals') # create a list of pdf files to be converted\n",
    "        for i in tnrange(len(list_of_files),desc='Converting pdf to txt'): # iterate throught the files on the list and install progress bar\n",
    "            if list_of_files[i].endswith(\".pdf\"): # check that file input is pdf file\n",
    "                parsedPDF=parser.from_file(os.getcwd()+'/parliament_practicals/'+list_of_files[i]) # parse pdf file\n",
    "                text_file = open(os.getcwd()+'/textfiles/'+list_of_files[i][:-4]+'.txt', 'a') # create new filename with extension .txt\n",
    "                text_file.write(parsedPDF[\"content\"]) # write parsed pdf to text\n",
    "                text_file.close() # close text file\n",
    "            else: # if file other than pdf continue loop\n",
    "                next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# If data are given for time saving purposes then set the following parameter to yes\n",
    "trg='no'\n",
    "# set link to parliament daily questions and answers reports\n",
    "page=\"http://www.parliament.uk/business/publications/written-questions-answers-statements/daily-reports/\" # set link to parliament daily questions and answers reports\n",
    "# set from which date to current date the function will download reports in YYYY-M-D format below\n",
    "start_date = date(2016, 6, 23)\n",
    "\n",
    "# Call function to either download the data or set current folder as working folder...please make sure that\n",
    "# if data are give then those should be stored on the folder: 'parliament_practicals'\n",
    "# We suggest to run the scraping function since it only takes 2minutes for downloading a year of reports\n",
    "data_control(page,start_date,trg) # call set data function\n",
    "convert_pdf_to_text(trg) # convert to pdf function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Defining and calling wrapped procedures for scraping, downloading and converting time-series exchange-rate shifts to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define function to download exchange rates to text file in folder xr\n",
    "def download_xr(html, trg):\n",
    "    if trg=='yes': # user input in case data are already given in appropriate format\n",
    "        print ('====Data were given====')\n",
    "    else:\n",
    "        try: # test if directory xr already exists otherwise make the directory\n",
    "            os.makedirs(os.getcwd()+'/xr') #make directory to downloaded files\n",
    "        except:\n",
    "            pass\n",
    "        soup_xr = BeautifulSoup(html)\n",
    "        xr = soup_xr.get_text()\n",
    "        #print(xr)\n",
    "        text_xr = open(os.getcwd()+'/xr/'+'exchangeRates'+'.txt', \"a\")\n",
    "        text_xr.write(xr)\n",
    "        text_xr.close()\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trg='no'\n",
    "html_page_xr = urllib.request.urlopen(\"http://www.bankofengland.co.uk/boeapps/iadb/fromshowcolumns.asp?Travel=NIxIRxSUx&FromSeries=1&ToSeries=50&DAT=RNG&FD=1&FM=Jan&FY=1963&TD=11&TM=Apr&TY=2017&VFD=Y&CSVF=TT&C=C8J&Filter=N&html.x=11&html.y=9\")\n",
    "download_xr(html_page_xr, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean exchange data download function and transform to pandas dataframe\n",
    "def clean_ex(file_path):\n",
    "    data = pd.read_csv(file_path,sep=\" t \",header=None, encoding=\"ISO-8859-1\") # load the text file\n",
    "    data = data.loc[6:, 0:2] # remove headers\n",
    "    data.reset_index(inplace=True,drop=True) # reset indices\n",
    "    date=data.iloc[::2] # extract odd rows\n",
    "    rate=data.iloc[1::2] # extract even rows\n",
    "    date.reset_index(inplace=True,drop=True) # reset indices\n",
    "    rate.reset_index(inplace=True,drop=True) # reset indices\n",
    "    x=pd.concat([date,rate],axis=1) # concatenate date and rates\n",
    "    if \n",
    "    x.columns=['Rate','Date'] # rename columns\n",
    "    x=x.dropna()\n",
    "    x['Date'] = pd.to_datetime(x['Date']) # convert date column to date\n",
    "    x[['Rate']] = x[['Rate']].apply(pd.to_numeric) #convert exchange rate to float\n",
    "    x.dtypes #check data types\n",
    "    x = x.set_index('Date').diff() # calculate [rate(t+1) - rate(t)]\n",
    "    return x # return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_xr = clean_ex(os.getcwd()+'/xr/exchangeRates.txt') # call clean function\n",
    "print(data_xr.head()) # print head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A: Select a dataset and make initial load and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe of filename - text with numbers and punctuation removed\n",
    "\n",
    "def remove_n_p(text): # function that removes punctuation and numbers as well lowercasing the text\n",
    "    text = re.sub(r'\\d+','', text) # remove numbers from texts with regular expressions <<<<<\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\\W+', ' ', text)# remove punctuation from texts with regular expressions <<<<<\n",
    "    text=text.lower() # lowercase the text\n",
    "    return text\n",
    "\n",
    "# we need a SparkSession to create DataFrames\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def make_dataFrame(dirPath): # make a dataFrame with filename and text \n",
    "    ft_RDD = sc.wholeTextFiles(dirPath) # add code to create an RDD with wholeTextFiles\n",
    "    spm_t_RDD = ft_RDD.map(lambda ft: (ft[0], remove_n_p(ft[1]))) # create RDD with filename and call remove_n_p function to text\n",
    "    rows_DF = spark.createDataFrame(spm_t_RDD,schema=['id','text']) # create a dataFrame - filename - text\n",
    "    return rows_DF\n",
    "\n",
    "file_text_df = make_dataFrame(os.getcwd()+'/textfiles') # assign dataframe to file_text_df\n",
    "#file_text_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperate dataset to train and test\n",
    "[train_DF,test_DF] = file_text_df.randomSplit([8.0,2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B: Implement a machine learning pipeline in Spark, including feature extractors, transformers, and/or selectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Step1: use tokenizer to split word into array and sql to select the filename - word_array created\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "#Step2: make hashTF sparse vector with maximum 500 features\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "#Step 3: feed hash vector to calculate idf\n",
    "idf = IDF(inputCol=\"hash_vector\", outputCol=\"idf\")\n",
    "\n",
    "#Step4: linear regression parameters\n",
    "lr = LinearRegression()\n",
    "\n",
    "#Step 5: configure pipeline with only hash vector\n",
    "pipeline_hash = Pipeline(stages=[tokenizer, hashingTF,lr])\n",
    "pipeline_hash_idf = Pipeline(stages=[tokenizer, hashingTF,lr])\n",
    "\n",
    "#Step 6: set parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [500]) \\\n",
    "    .addGrid(lr.regParam, [0.3]) \\\n",
    "    .addGrid(lr.maxIter, [50]) \\\n",
    "    .addGrid (lr.elasticNetParam,[0.8])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "train = pipeline_hash.fit(train_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "prediction = train.transform(test_DF)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C: Evaluate the performance of your pipeline using training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D: Implement a parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
